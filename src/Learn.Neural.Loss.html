<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><link rel="stylesheet" type="text/css" href="style.css" /><script type="text/javascript" src="highlight.js"></script></head><body><pre><span class="hs-pragma">{-# LANGUAGE DataKinds  #-}</span><span>
</span><a name="line-2"></a><span class="hs-pragma">{-# LANGUAGE GADTs      #-}</span><span>
</span><a name="line-3"></a><span class="hs-pragma">{-# LANGUAGE PolyKinds  #-}</span><span>
</span><a name="line-4"></a><span class="hs-pragma">{-# LANGUAGE RankNTypes #-}</span><span>
</span><a name="line-5"></a><span>
</span><a name="line-6"></a><span class="hs-keyword">module</span><span> </span><span class="hs-identifier">Learn</span><span class="hs-operator">.</span><span class="hs-identifier">Neural</span><span class="hs-operator">.</span><span class="hs-identifier">Loss</span><span> </span><span class="hs-special">(</span><span>
</span><a name="line-7"></a><span>    </span><a href="Learn.Neural.Loss.html#LossFunction"><span class="hs-identifier hs-type">LossFunction</span></a><span>
</span><a name="line-8"></a><span>  </span><span class="hs-special">,</span><span> </span><a href="Learn.Neural.Loss.html#crossEntropy"><span class="hs-identifier hs-var">crossEntropy</span></a><span>
</span><a name="line-9"></a><span>  </span><span class="hs-special">,</span><span> </span><a href="Learn.Neural.Loss.html#squaredError"><span class="hs-identifier hs-var">squaredError</span></a><span>
</span><a name="line-10"></a><span>  </span><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><a name="line-11"></a><span>
</span><a name="line-12"></a><span class="hs-keyword">import</span><span>           </span><span class="hs-identifier">GHC</span><span class="hs-operator">.</span><span class="hs-identifier">TypeLits</span><span>
</span><a name="line-13"></a><span class="hs-keyword">import</span><span>           </span><a href="Numeric.BLAS.html"><span class="hs-identifier">Numeric</span><span class="hs-operator">.</span><span class="hs-identifier">BLAS</span></a><span>
</span><a name="line-14"></a><span class="hs-keyword">import</span><span>           </span><span class="hs-identifier">Numeric</span><span class="hs-operator">.</span><span class="hs-identifier">Backprop</span><span>
</span><a name="line-15"></a><span>
</span><a name="line-16"></a><span class="hs-comment">-- type LossFunction s = forall b q. (BLAS b, Num (b s)) =&gt; b s -&gt; OpB q '[ b s ] '[ Scalar b ]</span><span>
</span><a name="line-17"></a><span class="hs-keyword">type</span><span> </span><a name="LossFunction"><a href="Learn.Neural.Loss.html#LossFunction"><span class="hs-identifier">LossFunction</span></a></a><span> </span><span class="hs-keyword">as</span><span> </span><a name="local-6989586621679303628"><a href="#local-6989586621679303628"><span class="hs-identifier">b</span></a></a><span> </span><span class="hs-glyph">=</span><span> </span><span class="hs-keyword">forall</span><span> </span><a name="local-6989586621679303629"><a href="#local-6989586621679303629"><span class="hs-identifier">s</span></a></a><span class="hs-operator">.</span><span> </span><span class="hs-identifier hs-type">Tuple</span><span> </span><span class="hs-keyword">as</span><span> </span><span class="hs-glyph">-&gt;</span><span> </span><span class="hs-identifier hs-type">OpB</span><span> </span><a href="#local-6989586621679303629"><span class="hs-identifier hs-type">s</span></a><span> </span><span class="hs-keyword">as</span><span> </span><span class="hs-char">'[ b ]

crossEntropy
    :: forall b n. (BLAS b, KnownNat n, Num (b '[n]))
    =&gt; LossFunction '[ b '[n] ] (Scalar b)
crossEntropy (I targ :&lt; &#216;) = bpOp . withInps $ \(r :&lt; &#216;) -&gt; do
    logR &lt;- tmapOp log ~$ (r :&lt; &#216;)
    res  &lt;- negate &lt;$&gt; (dotOp ~$ (logR :&lt; t :&lt; &#216;))
    only &lt;$&gt; bindVar res
  where
    t = constVar targ

squaredError
    :: (BLAS b, KnownNat n, Num (b '[n]))
    =&gt; LossFunction '[ b '[n] ] (Scalar b)
squaredError (I targ :&lt; &#216;) = bpOp . withInps $ \(r :&lt; &#216;) -&gt; do
    err  &lt;- bindVar $ r - t
    only &lt;$&gt; (dotOp ~$ (err :&lt; err :&lt; &#216;))
  where
    t = constVar targ

</span></pre></body></html>