-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Combinators and useful tools for ANNs using the backprop library
--   
--   See README.md
@package backprop-learn
@version 0.1.0.0


-- | Canonical strict tuples (and unit) with <a>Num</a> instances for usage
--   with <i>backprop</i>. This is here to solve the problem of orphan
--   instances in libraries and potential mismatched tuple types.
--   
--   If you are writing a library that needs to export <a>BVar</a>s of
--   tuples, consider using the tuples in this module so that your library
--   can have easy interoperability with other libraries using
--   <i>backprop</i>.
--   
--   Because of API decisions, <a>backprop</a> and <a>gradBP</a> only work
--   with things with <a>Num</a> instances. However, this disallows default
--   <tt>Prelude</tt> tuples (without orphan instances from packages like
--   <a>NumInstances</a>).
--   
--   Until tuples have <a>Num</a> instances in <i>base</i>, this module is
--   intended to be a workaround for situations where:
--   
--   This comes up often in cases where:
--   
--   <ol>
--   <li>A function wants to return more than one value (<tt><a>BVar</a> s
--   (<a>T2</a> a b)</tt></li>
--   <li>You want to uncurry a <a>BVar</a> function to use with
--   <a>backprop</a> and <a>gradBP</a>.</li>
--   <li>You want to use the useful <tt>Prism</tt>s automatically generated
--   by the lens library, which use tuples for multiple-constructor
--   fields.</li>
--   </ol>
--   
--   Only 2-tuples and 3-tuples are provided. Any more and you should
--   probably be using your own custom product types, with instances
--   automatically generated from something like
--   <a>one-liner-instances</a>.
--   
--   Lenses into the fields are provided, but they also work with
--   <a>_1</a>, <a>_2</a>, and <a>_3</a> from <a>Lens.Micro</a>. However,
--   note that these are incompatible with <a>_1</a>, <a>_2</a>, and
--   <a>_3</a> from <a>Control.Lens</a>.
--   
--   You can "construct" a <tt><a>BVar</a> s (<a>T2</a> a b)</tt> with
--   functions like <a>isoVar</a>.
module Data.Type.Tuple

-- | Unit ('()') with <a>Num</a>, <a>Fractional</a>, and <a>Floating</a>
--   instances.
--   
--   Be aware that the methods in its numerical instances are all
--   non-strict:
--   
--   <pre>
--   _ + _ = <a>T0</a>
--   <a>negate</a> _ = <a>T0</a>
--   <a>fromIntegral</a> _ = <a>T0</a>
--   </pre>
data T0
T0 :: T0

-- | Strict 2-tuple with <a>Num</a>, <a>Fractional</a>, and <a>Floating</a>
--   instances.
data T2 a b
T2 :: !a -> !b -> T2 a b

-- | Convert to a Haskell tuple.
--   
--   Forms an isomorphism with <a>tupT2</a>.
t2Tup :: T2 a b -> (a, b)

-- | Convert from Haskell tuple.
--   
--   Forms an isomorphism with <a>t2Tup</a>.
tupT2 :: (a, b) -> T2 a b

-- | Uncurry a function to take in a <a>T2</a> of its arguments
uncurryT2 :: (a -> b -> c) -> T2 a b -> c

-- | Curry a function taking a <a>T2</a> of its arguments
curryT2 :: (T2 a b -> c) -> a -> b -> c

-- | Lens into the first field of a <a>T2</a>. Also exported as <a>_1</a>
--   from <a>Lens.Micro</a>.
t2_1 :: Lens (T2 a b) (T2 a' b) a a'

-- | Lens into the second field of a <a>T2</a>. Also exported as <a>_2</a>
--   from <a>Lens.Micro</a>.
t2_2 :: Lens (T2 a b) (T2 a b') b b'

-- | Strict 3-tuple with a <a>Num</a>, <a>Fractional</a>, and
--   <a>Floating</a> instances.
data T3 a b c
T3 :: !a -> !b -> !c -> T3 a b c

-- | Convert to a Haskell tuple.
--   
--   Forms an isomorphism with <a>tupT3</a>.
t3Tup :: T3 a b c -> (a, b, c)

-- | Convert from Haskell tuple.
--   
--   Forms an isomorphism with <a>t3Tup</a>.
tupT3 :: (a, b, c) -> T3 a b c

-- | Lens into the first field of a <a>T3</a>. Also exported as <a>_1</a>
--   from <a>Lens.Micro</a>.
t3_1 :: Lens (T3 a b c) (T3 a' b c) a a'

-- | Lens into the second field of a <a>T3</a>. Also exported as <a>_2</a>
--   from <a>Lens.Micro</a>.
t3_2 :: Lens (T3 a b c) (T3 a b' c) b b'

-- | Lens into the third field of a <a>T3</a>. Also exported as <a>_3</a>
--   from <a>Lens.Micro</a>.
t3_3 :: Lens (T3 a b c) (T3 a b c') c c'

-- | Uncurry a function to take in a <a>T3</a> of its arguments
uncurryT3 :: (a -> b -> c -> d) -> T3 a b c -> d

-- | Curry a function taking a <a>T3</a> of its arguments
curryT3 :: (T3 a b c -> d) -> a -> b -> c -> d

-- | Strict inductive N-tuple with a <a>Num</a>, <a>Fractional</a>, and
--   <a>Floating</a> instances.
--   
--   It is basically "yet another HList", like the one found in
--   <a>Data.Type.Product</a> and many other locations on the haskell
--   ecosystem. Because it's inductively defined, it has O(n) random
--   indexing, but is efficient for zipping and mapping and other
--   sequential consumption patterns.
--   
--   It is provided because of its <a>Num</a> instance, making it useful
--   for <i>backproup</i>. Will be obsolete when <a>Product</a> gets
--   numerical instances.
data T :: [Type] -> Type
[TNil] :: T '[]
[:&] :: !a -> !(T as) -> T (a : as)

-- | Index into a <a>T</a>.
--   
--   <i>O(i)</i>
indexT :: Index as a -> T as -> a

-- | Extract a singleton <a>T</a>
--   
--   Forms an isomorphism with <a>onlyT</a>
tOnly :: T '[a] -> a

-- | A singleton <a>T</a>
--   
--   Forms an isomorphism with <a>tOnly</a>
onlyT :: a -> T '[a]

-- | Split a <a>T</a>. For splits known at compile-time, you can use
--   <a>known</a> to derive the <a>Length</a> automatically.
--   
--   Forms an isomorphism with <a>tAppend</a>.
tSplit :: Length as -> T (as ++ bs) -> (T as, T bs)

-- | Append two <a>T</a>s.
--   
--   Forms an isomorphism with <a>tSplit</a>.
tAppend :: T as -> T bs -> T (as ++ bs)
infixr 5 `tAppend`

-- | Convert a <a>T</a> to a <a>Tuple</a>.
--   
--   Forms an isomorphism with <a>prodT</a>.
tProd :: T as -> Tuple as

-- | Convert a <a>Tuple</a> to a <a>T</a>.
--   
--   Forms an isomorphism with <a>tProd</a>.
prodT :: Tuple as -> T as

-- | Lens into a given index of a <a>T</a>.
tIx :: Index as a -> Lens' (T as) a

-- | Lens into the head of a <a>T</a>
tHead :: Lens (T (a : as)) (T (b : as)) a b

-- | Lens into the tail of a <a>T</a>
tTail :: Lens (T (a : as)) (T (a : bs)) (T as) (T bs)

-- | Lens into the initial portion of a <a>T</a>. For splits known at
--   compile-time, you can use <a>known</a> to derive the <a>Length</a>
--   automatically.
tTake :: forall as bs cs. Length as -> Lens (T (as ++ bs)) (T (cs ++ bs)) (T as) (T cs)

-- | Lens into the ending portion of a <a>T</a>. For splits known at
--   compile-time, you can use <a>known</a> to derive the <a>Length</a>
--   automatically.
tDrop :: forall as bs cs. Length as -> Lens (T (as ++ bs)) (T (as ++ cs)) (T bs) (T cs)

-- | Initialize a <a>T</a> with a Rank-N value. Mostly used internally, but
--   provided in case useful.
--   
--   Must be used with <i>TypeApplications</i> to provide the Rank-N
--   constraint.
constT :: forall c as. ListC (c <$> as) => (forall a. c a => a) -> Length as -> T as

-- | Map over a <a>T</a> with a Rank-N function. Mostly used internally,
--   but provided in case useful.
--   
--   Must be used with <i>TypeApplications</i> to provide the Rank-N
--   constraint.
mapT :: forall c as. ListC (c <$> as) => (forall a. c a => a -> a) -> T as -> T as

-- | Map over a <a>T</a> with a Rank-N function. Mostly used internally,
--   but provided in case useful.
--   
--   Must be used with <i>TypeApplications</i> to provide the Rank-N
--   constraint.
zipT :: forall c as. ListC (c <$> as) => (forall a. c a => a -> a -> a) -> T as -> T as -> T as
instance (Data.Data.Data a, Data.Data.Data b, Data.Data.Data c) => Data.Data.Data (Data.Type.Tuple.T3 a b c)
instance GHC.Base.Functor (Data.Type.Tuple.T3 a b)
instance GHC.Generics.Generic (Data.Type.Tuple.T3 a b c)
instance (GHC.Classes.Ord a, GHC.Classes.Ord b, GHC.Classes.Ord c) => GHC.Classes.Ord (Data.Type.Tuple.T3 a b c)
instance (GHC.Classes.Eq a, GHC.Classes.Eq b, GHC.Classes.Eq c) => GHC.Classes.Eq (Data.Type.Tuple.T3 a b c)
instance (GHC.Read.Read a, GHC.Read.Read b, GHC.Read.Read c) => GHC.Read.Read (Data.Type.Tuple.T3 a b c)
instance (GHC.Show.Show a, GHC.Show.Show b, GHC.Show.Show c) => GHC.Show.Show (Data.Type.Tuple.T3 a b c)
instance (Data.Data.Data a, Data.Data.Data b) => Data.Data.Data (Data.Type.Tuple.T2 a b)
instance GHC.Base.Functor (Data.Type.Tuple.T2 a)
instance GHC.Generics.Generic (Data.Type.Tuple.T2 a b)
instance (GHC.Classes.Ord a, GHC.Classes.Ord b) => GHC.Classes.Ord (Data.Type.Tuple.T2 a b)
instance (GHC.Classes.Eq a, GHC.Classes.Eq b) => GHC.Classes.Eq (Data.Type.Tuple.T2 a b)
instance (GHC.Read.Read a, GHC.Read.Read b) => GHC.Read.Read (Data.Type.Tuple.T2 a b)
instance (GHC.Show.Show a, GHC.Show.Show b) => GHC.Show.Show (Data.Type.Tuple.T2 a b)
instance Data.Data.Data Data.Type.Tuple.T0
instance GHC.Generics.Generic Data.Type.Tuple.T0
instance GHC.Classes.Ord Data.Type.Tuple.T0
instance GHC.Classes.Eq Data.Type.Tuple.T0
instance GHC.Read.Read Data.Type.Tuple.T0
instance GHC.Show.Show Data.Type.Tuple.T0
instance Type.Family.List.ListC (GHC.Show.Show Type.Family.List.<$> as) => GHC.Show.Show (Data.Type.Tuple.T as)
instance Type.Family.List.ListC (GHC.Classes.Eq Type.Family.List.<$> as) => GHC.Classes.Eq (Data.Type.Tuple.T as)
instance (Type.Family.List.ListC (GHC.Classes.Eq Type.Family.List.<$> as), Type.Family.List.ListC (GHC.Classes.Ord Type.Family.List.<$> as)) => GHC.Classes.Ord (Data.Type.Tuple.T as)
instance Type.Family.List.ListC (Control.DeepSeq.NFData Type.Family.List.<$> as) => Control.DeepSeq.NFData (Data.Type.Tuple.T as)
instance Lens.Micro.Internal.Field1 (Data.Type.Tuple.T (a : as)) (Data.Type.Tuple.T (a : as)) a a
instance Lens.Micro.Internal.Field2 (Data.Type.Tuple.T (a : b : as)) (Data.Type.Tuple.T (a : b : as)) b b
instance Lens.Micro.Internal.Field3 (Data.Type.Tuple.T (a : b : c : as)) (Data.Type.Tuple.T (a : b : c : as)) c c
instance (Type.Class.Known.Known Data.Type.Length.Length as, Type.Family.List.ListC (GHC.Num.Num Type.Family.List.<$> as)) => GHC.Num.Num (Data.Type.Tuple.T as)
instance (Type.Class.Known.Known Data.Type.Length.Length as, Type.Family.List.ListC (GHC.Num.Num Type.Family.List.<$> as), Type.Family.List.ListC (GHC.Real.Fractional Type.Family.List.<$> as)) => GHC.Real.Fractional (Data.Type.Tuple.T as)
instance (Type.Class.Known.Known Data.Type.Length.Length as, Type.Family.List.ListC (GHC.Num.Num Type.Family.List.<$> as), Type.Family.List.ListC (GHC.Real.Fractional Type.Family.List.<$> as), Type.Family.List.ListC (GHC.Float.Floating Type.Family.List.<$> as)) => GHC.Float.Floating (Data.Type.Tuple.T as)
instance Type.Family.List.ListC (GHC.Base.Semigroup Type.Family.List.<$> as) => GHC.Base.Semigroup (Data.Type.Tuple.T as)
instance (Type.Class.Known.Known Data.Type.Length.Length as, Type.Family.List.ListC (GHC.Base.Semigroup Type.Family.List.<$> as), Type.Family.List.ListC (GHC.Base.Monoid Type.Family.List.<$> as)) => GHC.Base.Monoid (Data.Type.Tuple.T as)
instance (Type.Class.Known.Known Data.Type.Length.Length as, Type.Family.List.ListC (Data.Binary.Class.Binary Type.Family.List.<$> as)) => Data.Binary.Class.Binary (Data.Type.Tuple.T as)
instance Type.Family.List.ListC (Numeric.Backprop.Class.Backprop Type.Family.List.<$> as) => Numeric.Backprop.Class.Backprop (Data.Type.Tuple.T as)
instance (Type.Family.List.ListC (Numeric.Opto.Update.Additive Type.Family.List.<$> as), Type.Class.Known.Known Data.Type.Length.Length as) => Numeric.Opto.Update.Additive (Data.Type.Tuple.T as)
instance Numeric.Opto.Update.Scaling c a => Numeric.Opto.Update.Scaling c (Data.Type.Tuple.T '[a])
instance (Numeric.Opto.Update.Scaling c b, Numeric.Opto.Update.Scaling c (Data.Type.Tuple.T (a : as)), Numeric.Opto.Update.Additive a, Type.Family.List.ListC (Numeric.Opto.Update.Additive Type.Family.List.<$> as), Type.Class.Known.Known Data.Type.Length.Length as) => Numeric.Opto.Update.Scaling c (Data.Type.Tuple.T (b : a : as))
instance Numeric.Opto.Update.Metric c a => Numeric.Opto.Update.Metric c (Data.Type.Tuple.T '[a])
instance (Numeric.Opto.Update.Metric c b, Numeric.Opto.Update.Metric c (Data.Type.Tuple.T (a : as)), Numeric.Opto.Update.Scaling c (Data.Type.Tuple.T (a : as)), Numeric.Opto.Update.Additive a, Type.Family.List.ListC (Numeric.Opto.Update.Additive Type.Family.List.<$> as), Type.Class.Known.Known Data.Type.Length.Length as, GHC.Classes.Ord c, GHC.Float.Floating c) => Numeric.Opto.Update.Metric c (Data.Type.Tuple.T (b : a : as))
instance (Numeric.Opto.Ref.Ref m (Data.Type.Tuple.T as) v, Numeric.Opto.Update.Additive (Data.Type.Tuple.T as)) => Numeric.Opto.Update.AdditiveInPlace m v (Data.Type.Tuple.T as)
instance (Numeric.Opto.Ref.Ref m (Data.Type.Tuple.T as) v, Numeric.Opto.Update.Scaling c (Data.Type.Tuple.T as)) => Numeric.Opto.Update.ScalingInPlace m v c (Data.Type.Tuple.T as)
instance (Control.DeepSeq.NFData a, Control.DeepSeq.NFData b, Control.DeepSeq.NFData c) => Control.DeepSeq.NFData (Data.Type.Tuple.T3 a b c)
instance (Data.Binary.Class.Binary a, Data.Binary.Class.Binary b, Data.Binary.Class.Binary c) => Data.Binary.Class.Binary (Data.Type.Tuple.T3 a b c)
instance Data.Bifunctor.Bifunctor (Data.Type.Tuple.T3 a)
instance Lens.Micro.Internal.Field1 (Data.Type.Tuple.T3 a b c) (Data.Type.Tuple.T3 a' b c) a a'
instance Lens.Micro.Internal.Field2 (Data.Type.Tuple.T3 a b c) (Data.Type.Tuple.T3 a b' c) b b'
instance Lens.Micro.Internal.Field3 (Data.Type.Tuple.T3 a b c) (Data.Type.Tuple.T3 a b c') c c'
instance (GHC.Num.Num a, GHC.Num.Num b, GHC.Num.Num c) => GHC.Num.Num (Data.Type.Tuple.T3 a b c)
instance (GHC.Real.Fractional a, GHC.Real.Fractional b, GHC.Real.Fractional c) => GHC.Real.Fractional (Data.Type.Tuple.T3 a b c)
instance (GHC.Float.Floating a, GHC.Float.Floating b, GHC.Float.Floating c) => GHC.Float.Floating (Data.Type.Tuple.T3 a b c)
instance (GHC.Base.Semigroup a, GHC.Base.Semigroup b, GHC.Base.Semigroup c) => GHC.Base.Semigroup (Data.Type.Tuple.T3 a b c)
instance (GHC.Base.Monoid a, GHC.Base.Monoid b, GHC.Base.Monoid c) => GHC.Base.Monoid (Data.Type.Tuple.T3 a b c)
instance (Numeric.Backprop.Class.Backprop a, Numeric.Backprop.Class.Backprop b, Numeric.Backprop.Class.Backprop c) => Numeric.Backprop.Class.Backprop (Data.Type.Tuple.T3 a b c)
instance (Numeric.Opto.Update.Additive a, Numeric.Opto.Update.Additive b, Numeric.Opto.Update.Additive c) => Numeric.Opto.Update.Additive (Data.Type.Tuple.T3 a b c)
instance (Numeric.Opto.Update.Scaling c a, Numeric.Opto.Update.Scaling c b, Numeric.Opto.Update.Scaling c d) => Numeric.Opto.Update.Scaling c (Data.Type.Tuple.T3 a b d)
instance (Numeric.Opto.Update.Metric c a, Numeric.Opto.Update.Metric c b, Numeric.Opto.Update.Metric c d, GHC.Classes.Ord c, GHC.Float.Floating c) => Numeric.Opto.Update.Metric c (Data.Type.Tuple.T3 a b d)
instance (Numeric.Opto.Ref.Ref m (Data.Type.Tuple.T3 a b c) v, Numeric.Opto.Update.Additive a, Numeric.Opto.Update.Additive b, Numeric.Opto.Update.Additive c) => Numeric.Opto.Update.AdditiveInPlace m v (Data.Type.Tuple.T3 a b c)
instance (Numeric.Opto.Ref.Ref m (Data.Type.Tuple.T3 a b d) v, Numeric.Opto.Update.Scaling c a, Numeric.Opto.Update.Scaling c b, Numeric.Opto.Update.Scaling c d) => Numeric.Opto.Update.ScalingInPlace m v c (Data.Type.Tuple.T3 a b d)
instance (Control.DeepSeq.NFData a, Control.DeepSeq.NFData b) => Control.DeepSeq.NFData (Data.Type.Tuple.T2 a b)
instance (Data.Binary.Class.Binary a, Data.Binary.Class.Binary b) => Data.Binary.Class.Binary (Data.Type.Tuple.T2 a b)
instance Data.Bifunctor.Bifunctor Data.Type.Tuple.T2
instance Lens.Micro.Internal.Field1 (Data.Type.Tuple.T2 a b) (Data.Type.Tuple.T2 a' b) a a'
instance Lens.Micro.Internal.Field2 (Data.Type.Tuple.T2 a b) (Data.Type.Tuple.T2 a b') b b'
instance (GHC.Num.Num a, GHC.Num.Num b) => GHC.Num.Num (Data.Type.Tuple.T2 a b)
instance (GHC.Real.Fractional a, GHC.Real.Fractional b) => GHC.Real.Fractional (Data.Type.Tuple.T2 a b)
instance (GHC.Float.Floating a, GHC.Float.Floating b) => GHC.Float.Floating (Data.Type.Tuple.T2 a b)
instance (GHC.Base.Semigroup a, GHC.Base.Semigroup b) => GHC.Base.Semigroup (Data.Type.Tuple.T2 a b)
instance (GHC.Base.Monoid a, GHC.Base.Monoid b) => GHC.Base.Monoid (Data.Type.Tuple.T2 a b)
instance (Numeric.Backprop.Class.Backprop a, Numeric.Backprop.Class.Backprop b) => Numeric.Backprop.Class.Backprop (Data.Type.Tuple.T2 a b)
instance (Numeric.Opto.Update.Additive a, Numeric.Opto.Update.Additive b) => Numeric.Opto.Update.Additive (Data.Type.Tuple.T2 a b)
instance (Numeric.Opto.Update.Scaling c a, Numeric.Opto.Update.Scaling c b) => Numeric.Opto.Update.Scaling c (Data.Type.Tuple.T2 a b)
instance (Numeric.Opto.Update.Metric c a, Numeric.Opto.Update.Metric c b, GHC.Classes.Ord c, GHC.Float.Floating c) => Numeric.Opto.Update.Metric c (Data.Type.Tuple.T2 a b)
instance (Numeric.Opto.Ref.Ref m (Data.Type.Tuple.T2 a b) v, Numeric.Opto.Update.Additive a, Numeric.Opto.Update.Additive b) => Numeric.Opto.Update.AdditiveInPlace m v (Data.Type.Tuple.T2 a b)
instance (Numeric.Opto.Ref.Ref m (Data.Type.Tuple.T2 a b) v, Numeric.Opto.Update.Scaling c a, Numeric.Opto.Update.Scaling c b) => Numeric.Opto.Update.ScalingInPlace m v c (Data.Type.Tuple.T2 a b)
instance Control.DeepSeq.NFData Data.Type.Tuple.T0
instance Data.Binary.Class.Binary Data.Type.Tuple.T0
instance GHC.Num.Num Data.Type.Tuple.T0
instance GHC.Real.Fractional Data.Type.Tuple.T0
instance GHC.Float.Floating Data.Type.Tuple.T0
instance GHC.Base.Semigroup Data.Type.Tuple.T0
instance GHC.Base.Monoid Data.Type.Tuple.T0

module Data.Type.NonEmpty
data NETup :: NonEmpty Type -> Type
[NET] :: !a -> !(T as) -> NETup (a :| as)
netHead :: Lens (NETup (a :| as)) (NETup (b :| as)) a b
netTail :: Lens (NETup (a :| as)) (NETup (a :| bs)) (T as) (T bs)
unNet :: NETup (a :| as) -> (a, T as)
netT :: () => NETup a :| as -> T a : as

-- | Non-empty (and non-strict) list type.
data NonEmpty a
(:|) :: a -> [a] -> NonEmpty a
instance (Control.DeepSeq.NFData a, Type.Family.List.ListC (Control.DeepSeq.NFData Type.Family.List.<$> as)) => Control.DeepSeq.NFData (Data.Type.NonEmpty.NETup (a 'GHC.Base.:| as))
instance (GHC.Num.Num a, Type.Family.List.ListC (GHC.Num.Num Type.Family.List.<$> as), Type.Class.Known.Known Data.Type.Length.Length as) => GHC.Num.Num (Data.Type.NonEmpty.NETup (a 'GHC.Base.:| as))
instance (GHC.Real.Fractional a, Type.Family.List.ListC (GHC.Num.Num Type.Family.List.<$> as), Type.Family.List.ListC (GHC.Real.Fractional Type.Family.List.<$> as), Type.Class.Known.Known Data.Type.Length.Length as) => GHC.Real.Fractional (Data.Type.NonEmpty.NETup (a 'GHC.Base.:| as))
instance (GHC.Float.Floating a, Type.Family.List.ListC (GHC.Num.Num Type.Family.List.<$> as), Type.Family.List.ListC (GHC.Real.Fractional Type.Family.List.<$> as), Type.Family.List.ListC (GHC.Float.Floating Type.Family.List.<$> as), Type.Class.Known.Known Data.Type.Length.Length as) => GHC.Float.Floating (Data.Type.NonEmpty.NETup (a 'GHC.Base.:| as))
instance (Numeric.Opto.Update.Additive a, Numeric.Opto.Update.Additive (Data.Type.Tuple.T as)) => Numeric.Opto.Update.Additive (Data.Type.NonEmpty.NETup (a 'GHC.Base.:| as))
instance (Numeric.Opto.Update.Scaling c a, Numeric.Opto.Update.Scaling c (Data.Type.Tuple.T as)) => Numeric.Opto.Update.Scaling c (Data.Type.NonEmpty.NETup (a 'GHC.Base.:| as))
instance (Numeric.Opto.Update.Metric c a, Numeric.Opto.Update.Metric c (Data.Type.Tuple.T as), GHC.Classes.Ord c, GHC.Float.Floating c) => Numeric.Opto.Update.Metric c (Data.Type.NonEmpty.NETup (a 'GHC.Base.:| as))
instance (Numeric.Opto.Update.Additive a, Numeric.Opto.Update.Additive (Data.Type.Tuple.T as), Numeric.Opto.Ref.Ref m (Data.Type.NonEmpty.NETup (a 'GHC.Base.:| as)) v) => Numeric.Opto.Update.AdditiveInPlace m v (Data.Type.NonEmpty.NETup (a 'GHC.Base.:| as))
instance (Numeric.Opto.Update.Scaling s a, Numeric.Opto.Update.Scaling s (Data.Type.Tuple.T as), Numeric.Opto.Ref.Ref m (Data.Type.NonEmpty.NETup (a 'GHC.Base.:| as)) v) => Numeric.Opto.Update.ScalingInPlace m v s (Data.Type.NonEmpty.NETup (a 'GHC.Base.:| as))
instance (Numeric.Backprop.Class.Backprop a, Type.Family.List.ListC (Numeric.Backprop.Class.Backprop Type.Family.List.<$> as)) => Numeric.Backprop.Class.Backprop (Data.Type.NonEmpty.NETup (a 'GHC.Base.:| as))
instance (Data.Binary.Class.Binary a, Type.Family.List.ListC (Data.Binary.Class.Binary Type.Family.List.<$> as), Type.Class.Known.Known Data.Type.Length.Length as) => Data.Binary.Class.Binary (Data.Type.NonEmpty.NETup (a 'GHC.Base.:| as))

module Data.Type.Mayb
data Mayb :: (k -> Type) -> Maybe k -> Type
[N_] :: Mayb f  'Nothing
[J_] :: !(f a) -> Mayb f ( 'Just a)
fromJ_ :: Mayb f ( 'Just a) -> f a
maybToList :: Mayb f m -> Prod f (MaybeToList m)
listToMayb :: Prod f as -> Mayb f (ListToMaybe as)
data P :: k -> Type
[P] :: P a
type KnownMayb = Known (Mayb P)
knownMayb :: KnownMayb p => Mayb P p
zipMayb :: (forall a. f a -> g a -> h a) -> Mayb f m -> Mayb g m -> Mayb h m
zipMayb3 :: (forall a. f a -> g a -> h a -> i a) -> Mayb f m -> Mayb g m -> Mayb h m -> Mayb i m
class MaybeWit (c :: k -> Constraint) (m :: Maybe k)
maybeWit :: MaybeWit c m => Mayb (Wit1 c) m

-- | Map over a type-level <tt>Maybe</tt>.
splitTupMaybe :: forall f a b. (KnownMayb a, KnownMayb b) => (forall a' b'. (a ~  'Just a', b ~  'Just b') => f (T2 a' b') -> (f a', f b')) -> Mayb f (TupMaybe a b) -> (Mayb f a, Mayb f b)
tupMaybe :: forall f a b. () => (forall a' b'. (a ~  'Just a', b ~  'Just b') => f a' -> f b' -> f (T2 a' b')) -> Mayb f a -> Mayb f b -> Mayb f (TupMaybe a b)
boolMayb :: Boolean b -> Mayb P (BoolMayb b)
instance forall k (f :: k -> *) (m :: GHC.Base.Maybe k). Data.Type.Mayb.MaybeC GHC.Show.Show (f Type.Family.Maybe.<$> m) => GHC.Show.Show (Data.Type.Mayb.Mayb f m)
instance forall k (c :: k -> GHC.Types.Constraint) (m :: GHC.Base.Maybe k). (Data.Type.Mayb.MaybeC c m, Type.Class.Known.Known (Data.Type.Mayb.Mayb Data.Type.Mayb.P) m) => Data.Type.Mayb.MaybeWit c m
instance forall k1 (k2 :: k1). Type.Class.Known.Known Data.Type.Mayb.P k2
instance forall k (m :: GHC.Base.Maybe k) (f :: k -> *). (Type.Class.Known.Known (Data.Type.Mayb.Mayb Data.Type.Mayb.P) m, Data.Type.Mayb.MaybeC GHC.Num.Num (f Type.Family.Maybe.<$> m)) => GHC.Num.Num (Data.Type.Mayb.Mayb f m)
instance forall k (m :: GHC.Base.Maybe k) (f :: k -> *). (Type.Class.Known.Known (Data.Type.Mayb.Mayb Data.Type.Mayb.P) m, Data.Type.Mayb.MaybeC GHC.Num.Num (f Type.Family.Maybe.<$> m), Data.Type.Mayb.MaybeC GHC.Real.Fractional (f Type.Family.Maybe.<$> m)) => GHC.Real.Fractional (Data.Type.Mayb.Mayb f m)
instance forall k (f :: k -> *). Type.Class.Known.Known (Data.Type.Mayb.Mayb f) 'GHC.Base.Nothing
instance forall a1 (f :: a1 -> *) (a2 :: a1). Type.Class.Known.Known f a2 => Type.Class.Known.Known (Data.Type.Mayb.Mayb f) ('GHC.Base.Just a2)
instance Type.Class.Higher.Functor1 Data.Type.Mayb.Mayb
instance forall k (f :: k -> *) (a :: GHC.Base.Maybe k). Data.Type.Mayb.MaybeC Numeric.Backprop.Class.Backprop (f Type.Family.Maybe.<$> a) => Numeric.Backprop.Class.Backprop (Data.Type.Mayb.Mayb f a)

module Backprop.Learn.Model.Types
type ModelFunc p s a b = forall z. Reifies z W => Mayb (BVar z) p -> BVar z a -> Mayb (BVar z) s -> (BVar z b, Mayb (BVar z) s)
type ModelFuncStoch p s a b = forall m z. (PrimMonad m, Reifies z W) => Gen (PrimState m) -> Mayb (BVar z) p -> BVar z a -> Mayb (BVar z) s -> m (BVar z b, Mayb (BVar z) s)

-- | General parameterized model with potential state
data Model :: Maybe Type -> Maybe Type -> Type -> Type -> Type
[Model] :: {runLearn :: ModelFunc p s a b, runLearnStoch :: ModelFuncStoch p s a b} -> Model p s a b

-- | Construct a deterministic model, with no stochastic component.
modelD :: ModelFunc p s a b -> Model p s a b
type ModelFuncStateless p a b = forall z. Reifies z W => Mayb (BVar z) p -> BVar z a -> BVar z b
type ModelFuncStochStateless p a b = forall m z. (PrimMonad m, Reifies z W) => Gen (PrimState m) -> Mayb (BVar z) p -> BVar z a -> m (BVar z b)

-- | Parameterized model with no state
type ModelStateless p = Model p  'Nothing

-- | Construct a <a>ModelStateless</a>.
runLearnStateless :: () => ModelStateless p a b -> forall z. Reifies z W => Mayb BVar z p -> BVar z a -> BVar z b
runLearnStochStateless :: () => ModelStateless p a b -> forall (m :: * -> *) z. (PrimMonad m, Reifies z W) => Gen PrimState m -> Mayb BVar z p -> BVar z a -> m BVar z b

-- | Construct a deterministic stateless model, with no stochastic
--   component.
modelStatelessD :: ModelFuncStateless p a b -> ModelStateless p a b
type BFunc a b = forall z. Reifies z W => BVar z a -> BVar z b
type BFuncStoch a b = forall m z. (PrimMonad m, Reifies z W) => Gen (PrimState m) -> BVar z a -> m (BVar z b)
type Func = Model  'Nothing  'Nothing

-- | Unparameterized model with no state
runFunc :: () => Func a b -> forall z. Reifies z W => BVar z a -> BVar z b
runFuncStoch :: () => Func a b -> forall (m :: * -> *) z. (PrimMonad m, Reifies z W) => Gen PrimState m -> BVar z a -> m BVar z b

-- | Construct an deterministic unparameterized stateless model, with no
--   stochastic component.
funcD :: BFunc a b -> Func a b
type ModelFuncM m p s a b = forall z. Reifies z W => Mayb (BVar z) p -> BVar z a -> Mayb (BVar z) s -> m (BVar z b, Mayb (BVar z) s)
withModelFunc0 :: (forall m. Monad m => ModelFuncM m p s a b) -> Model p s a b
withModelFunc :: (forall m. Monad m => ModelFuncM m p s a b -> ModelFuncM m q t c d) -> Model p s a b -> Model q t c d
withModelFunc2 :: (forall m. Monad m => ModelFuncM m p s a b -> ModelFuncM m q t c d -> ModelFuncM m r u e f) -> Model p s a b -> Model q t c d -> Model r u e f
data Mayb :: (k -> Type) -> Maybe k -> Type
[N_] :: Mayb f  'Nothing
[J_] :: !(f a) -> Mayb f ( 'Just a)
fromJ_ :: Mayb f ( 'Just a) -> f a
type KnownMayb = Known (Mayb P)
knownMayb :: KnownMayb p => Mayb P p
newtype I a
I :: a -> I a
[getI] :: I a -> a

-- | Helper type family for HKD data types. See
--   <a>http://reasonablypolymorphic.com/blog/higher-kinded-data</a>
instance Control.Category.Category (Backprop.Learn.Model.Types.Model p s)

module Backprop.Learn.Model.Parameter

-- | Fix a part of a parameter as constant, preventing backpropagation
--   through it and not training it.
--   
--   Treats a <tt>pq</tt> parameter as essentialyl a <tt>(p, q)</tt>,
--   witnessed through the split and join functions.
--   
--   Takes the fixed value of <tt>q</tt>, as well as a stochastic mode
--   version with fixed distribution.
deParam :: forall p q pq s a b. (Backprop p, Backprop q, Backprop pq) => (pq -> (p, q)) -> (p -> q -> pq) -> q -> (forall m. (PrimMonad m) => Gen (PrimState m) -> m q) -> Model ( 'Just pq) s a b -> Model ( 'Just p) s a b

-- | <a>deParam</a>, but with no special stochastic mode version.
deParamD :: (Backprop p, Backprop q, Backprop pq) => (pq -> (p, q)) -> (p -> q -> pq) -> q -> Model ( 'Just pq) s a b -> Model ( 'Just p) s a b

-- | Pre-applies a function to a parameter before a model sees it.
--   Essentially something like <tt>lmap</tt> for parameters.
--   
--   Takes a determinstic function and also a stochastic function for
--   stochastic mode.
reParam :: (forall z. Reifies z W => Mayb (BVar z) q -> Mayb (BVar z) p) -> (forall m z. (PrimMonad m, Reifies z W) => Gen (PrimState m) -> Mayb (BVar z) q -> m (Mayb (BVar z) p)) -> Model p s a b -> Model q s a b

-- | <a>reParam</a>, but with no special stochastic mode function.
reParamD :: (forall z. Reifies z W => Mayb (BVar z) q -> Mayb (BVar z) p) -> Model p s a b -> Model q s a b

-- | Give an unparameterized model a "dummy" parameter. Useful for usage
--   with combinators like <a>.</a> from that require all input models to
--   share a common parameterization.
dummyParam :: Model  'Nothing s a b -> Model p s a b

module Backprop.Learn.Model.State

-- | Make a model stateless by converting the state to a trained parameter,
--   and dropping the modified state from the result.
--   
--   One of the ways to make a model stateless for training purposes.
--   Useful when used after <tt>Unroll</tt>. See <tt>DeState</tt>, as well.
--   
--   Its parameters are:
--   
--   <ul>
--   <li>If the input has no parameters, just the initial state.</li>
--   <li>If the input has a parameter, a <a>T2</a> of that parameter and
--   initial state.</li>
--   </ul>
trainState :: forall p s a b. (KnownMayb p, KnownMayb s, MaybeC Backprop p, MaybeC Backprop s) => Model p s a b -> Model (TupMaybe p s)  'Nothing a b

-- | Make a model stateless by pre-applying a fixed state (or a stochastic
--   one with fixed stribution) and dropping the modified state from the
--   result.
--   
--   One of the ways to make a model stateless for training purposes.
--   Useful when used after <tt>Unroll</tt>. See <tt>TrainState</tt>, as
--   well.
deState :: s -> (forall m. PrimMonad m => Gen (PrimState m) -> m s) -> Model p ( 'Just s) a b -> Model p  'Nothing a b

-- | <a>deState</a>, except the state is always the same even in stochastic
--   mode.
deStateD :: s -> Model p ( 'Just s) a b -> Model p  'Nothing a b

-- | <a>deState</a> with a constant state of 0.
zeroState :: Num s => Model p ( 'Just s) a b -> Model p  'Nothing a b

-- | Give a stateless model a "dummy" state. For now, useful for using with
--   combinators like <a>deState</a> that require state. However,
--   <a>deState</a> could also be made more lenient (to accept non stateful
--   models) in the future.
--   
--   Also useful for usage with combinators like <a>.</a> from
--   <a>Control.Category</a> that requires all input models to share common
--   state.
dummyState :: forall s p a b. () => Model p  'Nothing a b -> Model p s a b

-- | Unroll a (usually) stateful model into one taking a vector of
--   sequential inputs.
--   
--   Basically applies the model to every item of input and returns all of
--   the results, but propagating the state between every step.
--   
--   Useful when used before <a>trainState</a> or <a>deState</a>. See
--   <tt>unrollTrainState</tt> and <tt>unrollDeState</tt>.
--   
--   Compare to <tt>feedbackTrace</tt>, which, instead of receiving a
--   vector of sequential inputs, receives a single input and uses its
--   output as the next input.
unroll :: (Traversable t, Backprop a, Backprop b, Backprop (t b)) => Model p s a b -> Model p s (t a) (t b)

-- | Version of <a>unroll</a> that only keeps the "final" result, dropping
--   all of the intermediate results.
--   
--   Turns a stateful model into one that runs the model repeatedly on
--   multiple inputs sequentially and outputs the final result after seeing
--   all items.
--   
--   Note will be partial if given an empty sequence.
unrollFinal :: (Traversable t, Backprop a) => Model p s a b -> Model p s (t a) b

-- | Fix a part of a parameter of a model to be (a function of) the
--   <i>previous</i> ouput of the model itself.
--   
--   Essentially, takes a &lt;math&gt; into a <i>stateful</i> &lt;math&gt;,
--   where the Y is given by a function of the <i>previous output</i> of
--   the model.
--   
--   Essentially makes a model "recurrent": it receives its previous output
--   as input.
--   
--   See <tt>fcr</tt> for an application.
recurrent :: forall p s ab a b c. (KnownMayb s, MaybeC Backprop s, Backprop ab, Backprop a, Backprop b) => (ab -> (a, b)) -> (a -> b -> ab) -> BFunc c b -> Model p s ab c -> Model p (TupMaybe s ( 'Just b)) a c

module Backprop.Learn.Model.Function

-- | Binary step / heaviside step
--   
--   To use with vectors (<a>R</a>), use <a>vmap'</a>.
--   
--   &lt;math&gt;
step :: (Ord a, Num a) => a -> a

-- | Logistic function
--   
--   &lt;math&gt;
logistic :: Floating a => a -> a

-- | Softsign activation function
--   
--   &lt;math&gt;
softsign :: Fractional a => a -> a

-- | Rectified linear unit.
--   
--   To use with vectors (<a>R</a>), use <a>vmap'</a>.
--   
--   &lt;math&gt;
--   
--   <pre>
--   <a>reLU</a> = <a>preLU</a> 0
--   </pre>
reLU :: (Num a, Ord a) => a -> a

-- | SoftPlus
--   
--   &lt;math&gt;
softPlus :: Floating a => a -> a

-- | Bent identity
--   
--   &lt;math&gt;
bentIdentity :: Floating a => a -> a

-- | Sigmoid-weighted linear unit. Multiply <a>logistic</a> by its input.
--   
--   &lt;math&gt;
siLU :: Floating a => a -> a

-- | SoftExponential
--   
--   To use with vectors (<a>R</a>), use <a>vmap'</a>.
--   
--   &lt;math&gt;
softExponential :: (Floating a, Ord a) => a -> a -> a

-- | Sinc
--   
--   &lt;math&gt;
sinc :: (Floating a, Eq a) => a -> a

-- | Gaussian
--   
--   &lt;math&gt;
gaussian :: Floating a => a -> a
tanh :: Floating a => a -> a
atan :: Floating a => a -> a
sin :: Floating a => a -> a

-- | Note: if possible, use the potentially much more performant
--   <a>vmap'</a>.
vmap :: (Reifies s W, KnownNat n) => BVar s ℝ -> BVar s ℝ -> BVar s R n -> BVar s R n

-- | <a>vmap</a>, but potentially more performant. Only usable if the
--   mapped function does not depend on any external <a>BVar</a>s.
vmap' :: (Reifies s W, Num vec n, Storable field, Sized field vec n Vector) => forall s'. Reifies s' W => BVar s' field -> BVar s' field -> BVar s vec n -> BVar s vec n

-- | Usable with functions like <a>*</a>, <a>isru</a>, etc. to turn them
--   into a form usable with <tt>PFP</tt>:
--   
--   <pre>
--   <a>liftUniform</a> (<a>*</a>)  :: <a>BVar</a> s <a>Double</a> -&gt; BVar s (<a>R</a> n) -&gt; BVar s (R n)
--   liftUniform <a>isru</a> :: BVar s Double -&gt; BVar s (R n) -&gt; BVar s (R n)
--   </pre>
--   
--   Basically turns a parmaeterized function on individual elements of
--   into one that shares the same parameter across all elements of the
--   vector.
liftUniform :: (Reifies s W, KnownNat n) => (BVar s (R n) -> r) -> BVar s Double -> r

-- | Inverse square root unit
--   
--   &lt;math&gt;
--   
--   See <a>liftUniform</a> to make this compatible with <tt>PFP</tt>.
--   
--   You can also just use this after partially applying it, to fix the
--   parameter (and not have it trained).
isru :: Floating a => a -> a -> a

-- | Parametric rectified linear unit
--   
--   To use with vectors (<a>R</a>), use <a>vmap'</a>.
--   
--   If scaling parameter is a fixed (and not learned) parameter, this is
--   typically called a leaky recitified linear unit (typically with α =
--   0.01).
--   
--   To use as a learned parameter:
--   
--   <pre>
--   <a>vmap</a> . <a>preLU</a> :: <a>BVar</a> s Double -&gt; <a>BVar</a> s (<a>R</a> n) -&gt; BVar s (R n)
--   </pre>
--   
--   This can be give directly to <tt>PFP</tt>.
--   
--   To fix the paramater ("leaky"), just partially apply a parameter:
--   
--   <pre>
--   <a>preLU</a> 0.01           :: <a>BVar</a> s (<a>R</a> n) -&gt; BVar s (R n)
--   preLU (<a>realToFrac</a> α) :: BVar s (R n) -&gt; BVar s (R n)
--   </pre>
--   
--   See also <tt>rreLU</tt>.
--   
--   &lt;math&gt;
preLU :: (Num a, Ord a) => a -> a -> a

-- | S-shaped rectified linear activiation unit
--   
--   See <a>sreLUPFP</a> for an uncurried and uniformly lifted version
--   usable with <tt>PFP</tt>.
--   
--   &lt;math&gt;
sreLU :: (Num a, Ord a) => a -> a -> a -> a -> a -> a

-- | An uncurried and uniformly lifted version of <a>sreLU</a> directly
--   usable with <tt>PFP</tt>.
sreLUPFP :: (KnownNat n, Reifies s W) => BVar s (T2 (T2 Double Double) (T2 Double Double)) -> BVar s (R n) -> BVar s (R n)

-- | Exponential linear unit
--   
--   To use with vectors (<a>R</a>), use <a>vmap'</a>.
--   
--   To use as a learned parameter:
--   
--   <pre>
--   <a>vmap</a> . <a>eLU</a> :: <a>BVar</a> s Double -&gt; <a>BVar</a> s (<a>R</a> n) -&gt; BVar s (R n)
--   </pre>
--   
--   This can be give directly to <tt>PFP</tt>.
--   
--   To fix the paramater, just partially apply a parameter:
--   
--   <pre>
--   <a>vmap'</a> (<a>eLU</a> 0.01) :: <a>BVar</a> s (<a>R</a> n) -&gt; BVar s (R n)
--   </pre>
--   
--   &lt;math&gt;
eLU :: (Floating a, Ord a) => a -> a -> a

-- | Inverse square root linear unit
--   
--   To use with vectors (<a>R</a>), use <a>vmap'</a>.
--   
--   &lt;math&gt;
isrLU :: (Floating a, Ord a) => a -> a -> a

-- | Adaptive piecewise linear activation unit
--   
--   See <a>aplPFP</a> for an uncurried version usable with <tt>PFP</tt>.
--   
--   &lt;math&gt;
apl :: (KnownNat n, KnownNat m, Reifies s W) => BVar s (L n m) -> BVar s (L n m) -> BVar s (R m) -> BVar s (R m)

-- | <a>apl</a> uncurried, to be directly usable with <tt>PFP</tt>.
aplPFP :: (KnownNat n, KnownNat m, Reifies s W) => BVar s (T2 (L n m) (L n m)) -> BVar s (R m) -> BVar s (R m)

-- | Softmax normalizer
softMax :: (KnownNat i, Reifies s W) => BVar s (R i) -> BVar s (R i)

-- | Maximum of vector.
--   
--   Compare to <a>norm_InfV</a>, which gives the maximum absolute value.
maxout :: (KnownNat n, Reifies s W) => BVar s (R n) -> BVar s Double

-- | Keep only the top <tt>k</tt> values, and zero out all of the rest.
--   
--   Useful for postcomposing in between layers (with a logistic function
--   before) to encourage the number of "activated" neurons is kept to be
--   around <tt>k</tt>. Used in k-Sprase autoencoders (see
--   <tt>KAutoencoder</tt>).
--   
--   
--   <a>http://www.ericlwilkinson.com/blog/2014/11/19/deep-learning-sparse-autoencoders</a>
kSparse :: forall n s. (Reifies s W, KnownNat n) => Int -> BVar s (R n) -> BVar s (R n)

module Backprop.Learn.Model.Stochastic

-- | Dropout layer. Parameterized by dropout percentage (should be between
--   0 and 1).
--   
--   0 corresponds to no dropout, 1 corresponds to complete dropout of all
--   nodes every time.
dropout :: KnownNat n => Double -> Model  'Nothing  'Nothing (R n) (R n)

-- | Random leaky rectified linear unit
rreLU :: (ContGen d, Mean d, KnownNat n) => d -> Model  'Nothing  'Nothing (R n) (R n)

-- | Inject random noise. Usually used between neural network layers, or at
--   the very beginning to pre-process input.
--   
--   In non-stochastic mode, this adds the mean of the distribution.
injectNoise :: (ContGen d, Mean d, Fractional a) => d -> Model  'Nothing  'Nothing a a

-- | Multply by random noise. Can be used to implement dropout-like
--   behavior.
--   
--   In non-stochastic mode, this scales by the mean of the distribution.
applyNoise :: (ContGen d, Mean d, Fractional a) => d -> Model  'Nothing  'Nothing a a

-- | <a>injectNoise</a> lifted to <a>R</a>
injectNoiseR :: (ContGen d, Mean d, KnownNat n) => d -> Model  'Nothing  'Nothing (R n) (R n)

-- | <a>applyNoise</a> lifted to <a>R</a>
applyNoiseR :: (ContGen d, Mean d, KnownNat n) => d -> Model  'Nothing  'Nothing (R n) (R n)

module Backprop.Learn.Model.Combinator

-- | Compose two <a>Model</a>s one after the other, in reverse composition
--   order
(~>) :: forall p q s t a b c. (KnownMayb p, KnownMayb q, KnownMayb s, KnownMayb t, MaybeC Backprop p, MaybeC Backprop q, MaybeC Backprop s, MaybeC Backprop t) => Model q t a b -> Model p s b c -> Model (TupMaybe p q) (TupMaybe s t) a c
infixr 8 ~>

-- | Compose two <a>Model</a>s one after the other.
(<~) :: forall p q s t a b c. (KnownMayb p, KnownMayb q, KnownMayb s, KnownMayb t, MaybeC Backprop p, MaybeC Backprop q, MaybeC Backprop s, MaybeC Backprop t) => Model p s b c -> Model q t a b -> Model (TupMaybe p q) (TupMaybe s t) a c
infixr 8 <~

-- | <a>Model</a> where parameters and states are heterogeneous lists
--   (<a>T</a>), making for more seamless composition.
type LModel ps ss a b = Model ( 'Just (T ps)) ( 'Just (T ss)) a b

-- | <tt>Cons</tt> a model to the end of a chain of <a>LModel</a>
--   compositions. Can be used with <a>nilLM</a>.
(#:) :: forall p ps s ss a b c. (MaybeC Backprop p, ListC (Backprop <$> ps), MaybeC Backprop s, ListC (Backprop <$> ss), KnownMayb p, KnownMayb s) => Model p s b c -> LModel ps ss a b -> LModel (MaybeToList p ++ ps) (MaybeToList s ++ ss) a c
infixr 5 #:

-- | Identity of <a>#++</a>
nilLM :: Model ( 'Just (T '[])) ( 'Just (T '[])) a a

-- | Compose two <a>LModel</a>s
(#++) :: forall ps qs ss ts a b c. (ListC (Backprop <$> ps), ListC (Backprop <$> qs), ListC (Backprop <$> ss), ListC (Backprop <$> ts), ListC (Backprop <$> (ss ++ ts)), Known Length ps, Known Length ss, Known Length ts) => LModel ps ss b c -> LModel qs ts a b -> LModel (ps ++ qs) (ss ++ ts) a c
infixr 5 #++

-- | Lift a normal <a>Model</a> to a <a>LModel</a> with a singleton list
--   parameter/state if '<a>Just</a>, or an empty list if '<a>Nothing</a>.
--   Essentially prepares a model to be used with <tt>#~</tt> and
--   <a>nilLM</a>.
liftLM :: forall p s a b. (KnownMayb p, MaybeC Backprop p, KnownMayb s, MaybeC Backprop s) => Model p s a b -> LModel (MaybeToList p) (MaybeToList s) a b

-- | Return a model that loops a model ("feed") repeatedly onto itself,
--   with a model to provide the back loop.
feedback :: forall p q s t a b. (KnownMayb p, KnownMayb q, KnownMayb s, KnownMayb t, MaybeC Backprop p, MaybeC Backprop q, MaybeC Backprop s, MaybeC Backprop t) => Int -> Model p s a b -> Model q t b a -> Model (TupMaybe p q) (TupMaybe s t) a b

-- | <a>feedback</a>, but tracing and observing all of the intermediate
--   values.
feedbackTrace :: forall n p q s t a b. (KnownMayb p, KnownMayb q, KnownMayb s, KnownMayb t, MaybeC Backprop p, MaybeC Backprop q, MaybeC Backprop s, MaybeC Backprop t, KnownNat n, Backprop b) => Model p s a b -> Model q t b a -> Model (TupMaybe p q) (TupMaybe s t) a (ABP (Vector n) b)

module Backprop.Learn.Loss
type Loss a = forall s. Reifies s W => a -> BVar s a -> BVar s Double
crossEntropy :: KnownNat n => Loss (R n)
squaredError :: Loss Double
absError :: Loss Double
totalSquaredError :: (Backprop (t Double), Num (t Double), Foldable t, Functor t) => Loss (t Double)
squaredErrorV :: KnownNat n => Loss (R n)

-- | Scale the result of a loss function.
scaleLoss :: Double -> Loss a -> Loss a
sumLoss :: (Traversable t, Applicative t, Backprop a) => Loss a -> Loss (t a)
sumLossDecay :: forall n a. (KnownNat n, Backprop a) => Double -> Loss a -> Loss (Vector n a)
lastLoss :: (KnownNat (n + 1), Backprop a) => Loss a -> Loss (Vector (n + 1) a)
zipLoss :: (Traversable t, Applicative t, Backprop a) => t Double -> Loss a -> Loss (t a)

-- | Lift and sum a loss function over the components of a <a>T2</a>.
t2Loss :: (Backprop a, Backprop b) => Loss a -> Loss b -> Loss (T2 a b)

-- | Lift and sum a loss function over the components of a <a>T3</a>.
t3Loss :: (Backprop a, Backprop b, Backprop c) => Loss a -> Loss b -> Loss c -> Loss (T3 a b c)

-- | A regularizer on parameters
type Regularizer p = forall s. Reifies s W => BVar s p -> BVar s Double

-- | L2 regularization
--   
--   &lt;math&gt;
l2Reg :: (Metric Double p, Backprop p) => Double -> Regularizer p

-- | L1 regularization
--   
--   &lt;math&gt;
l1Reg :: (Num p, Metric Double p, Backprop p) => Double -> Regularizer p

-- | No regularization
noReg :: Regularizer p

-- | Add together two regularizers
addReg :: Regularizer p -> Regularizer p -> Regularizer p

-- | Scale a regularizer's influence
scaleReg :: Double -> Regularizer p -> Regularizer p

module Backprop.Learn.Initialize

-- | Class for types that are basically a bunch of <a>Double</a>s, which
--   can be initialized with a given identical and independent
--   distribution.
class Initialize p
initialize :: (Initialize p, ContGen d, PrimMonad m) => d -> Gen (PrimState m) -> m p
initialize :: (Initialize p, ADTRecord p, Constraints p Initialize, ContGen d, PrimMonad m) => d -> Gen (PrimState m) -> m p

-- | <a>initialize</a> for any instance of <tt>Generic</tt>.
gInitialize :: (ADTRecord p, Constraints p Initialize, ContGen d, PrimMonad m) => d -> Gen (PrimState m) -> m p
initializeNormal :: (Initialize p, PrimMonad m) => Double -> Gen (PrimState m) -> m p

-- | <a>initialize</a> definition if <tt>p</tt> is a single number.
initializeSingle :: (ContGen d, PrimMonad m, Fractional p) => d -> Gen (PrimState m) -> m p
instance Backprop.Learn.Initialize.Initialize GHC.Types.Double
instance Backprop.Learn.Initialize.Initialize GHC.Types.Float
instance Backprop.Learn.Initialize.Initialize a => Backprop.Learn.Initialize.Initialize (Data.Complex.Complex a)
instance Backprop.Learn.Initialize.Initialize Data.Type.Tuple.T0
instance (Backprop.Learn.Initialize.Initialize a, Backprop.Learn.Initialize.Initialize b) => Backprop.Learn.Initialize.Initialize (Data.Type.Tuple.T2 a b)
instance (Backprop.Learn.Initialize.Initialize a, Backprop.Learn.Initialize.Initialize b, Backprop.Learn.Initialize.Initialize c) => Backprop.Learn.Initialize.Initialize (Data.Type.Tuple.T3 a b c)
instance (Type.Family.List.ListC (Backprop.Learn.Initialize.Initialize Type.Family.List.<$> as), Type.Class.Known.Known Data.Type.Length.Length as) => Backprop.Learn.Initialize.Initialize (Data.Type.Tuple.T as)
instance (Backprop.Learn.Initialize.Initialize a, Type.Family.List.ListC (Backprop.Learn.Initialize.Initialize Type.Family.List.<$> as), Type.Class.Known.Known Data.Type.Length.Length as) => Backprop.Learn.Initialize.Initialize (Data.Type.NonEmpty.NETup (a 'GHC.Base.:| as))
instance Backprop.Learn.Initialize.Initialize ()
instance (Backprop.Learn.Initialize.Initialize a, Backprop.Learn.Initialize.Initialize b) => Backprop.Learn.Initialize.Initialize (a, b)
instance (Backprop.Learn.Initialize.Initialize a, Backprop.Learn.Initialize.Initialize b, Backprop.Learn.Initialize.Initialize c) => Backprop.Learn.Initialize.Initialize (a, b, c)
instance (Data.Vector.Generic.Base.Vector v a, GHC.TypeNats.KnownNat n, Backprop.Learn.Initialize.Initialize a) => Backprop.Learn.Initialize.Initialize (Data.Vector.Generic.Sized.Internal.Vector v n a)
instance GHC.TypeNats.KnownNat n => Backprop.Learn.Initialize.Initialize (Internal.Static.R n)
instance GHC.TypeNats.KnownNat n => Backprop.Learn.Initialize.Initialize (Internal.Static.C n)
instance (GHC.TypeNats.KnownNat n, GHC.TypeNats.KnownNat m) => Backprop.Learn.Initialize.Initialize (Internal.Static.L n m)
instance (GHC.TypeNats.KnownNat n, GHC.TypeNats.KnownNat m) => Backprop.Learn.Initialize.Initialize (Internal.Static.M n m)

module Backprop.Learn.Model.Regression
linReg :: (KnownNat i, KnownNat o) => Model ( 'Just (LRp i o))  'Nothing (R i) (R o)
logReg :: (KnownNat i, KnownNat o) => Model ( 'Just (LRp i o))  'Nothing (R i) (R o)

-- | Linear Regression parameter
data LRp i o
LRp :: !(R o) -> !(L o i) -> LRp i o
[_lrAlpha] :: LRp i o -> !(R o)
[_lrBeta] :: LRp i o -> !(L o i)
lrAlpha :: forall i_a191h o_a191i. Lens' (LRp i_a191h o_a191i) (R o_a191i)
lrBeta :: forall i_a191h o_a191i i_a19fg. Lens (LRp i_a191h o_a191i) (LRp i_a19fg o_a191i) (L o_a191i i_a191h) (L o_a191i i_a19fg)
runLRp :: (KnownNat i, KnownNat o, Reifies s W) => BVar s (LRp i o) -> BVar s (R i) -> BVar s (R o)

-- | Adjust an <a>LRp</a> to take extra inputs, initialized randomly.
--   
--   Initial contributions to each output is randomized.
expandInput :: (PrimMonad m, ContGen d, KnownNat i, KnownNat j, KnownNat o) => LRp i o -> d -> Gen (PrimState m) -> m (LRp (i + j) o)

-- | Adjust an <a>LRp</a> to return extra ouputs, initialized randomly
expandOutput :: (PrimMonad m, ContGen d, KnownNat i, KnownNat o, KnownNat p) => LRp i o -> d -> Gen (PrimState m) -> m (LRp i (o + p))

-- | Premute (or remove) inputs
--   
--   Removed inputs will simply have their contributions removed from each
--   output.
reshapeInput :: KnownNat i => Vector i' (Finite i) -> LRp i o -> LRp i' o

-- | Premute (or remove) outputs
reshapeOutput :: KnownNat o => Vector o' (Finite o) -> LRp i o -> LRp i o'
arima :: forall p d q. (KnownNat p, KnownNat d, KnownNat q) => Model ( 'Just (ARIMAp p q)) ( 'Just (ARIMAs p d q)) Double Double
autoregressive :: KnownNat p => Model ( 'Just (ARIMAp p 0)) ( 'Just (ARIMAs p 0 0)) Double Double
movingAverage :: KnownNat q => Model ( 'Just (ARIMAp 0 q)) ( 'Just (ARIMAs 0 0 q)) Double Double
arma :: (KnownNat p, KnownNat q) => Model ( 'Just (ARIMAp p q)) ( 'Just (ARIMAs p 0 q)) Double Double

-- | <tt>ARIMA</tt> parmaeters
data ARIMAp :: Nat -> Nat -> Type
[ARIMAp] :: {_arimaPhi :: !(R p), _arimaTheta :: !(R q), _arimaConstant :: !Double} -> ARIMAp p q

-- | <tt>ARIMA</tt> state
data ARIMAs :: Nat -> Nat -> Nat -> Type
[ARIMAs] :: {_arimaYPred :: !Double, _arimaYHist :: !(R (p + d)), _arimaEHist :: !(R q)} -> ARIMAs p d q
arimaPhi :: forall a_a19l6 b_a19l7 a_a19Hg. Lens (ARIMAp a_a19l6 b_a19l7) (ARIMAp a_a19Hg b_a19l7) (R a_a19l6) (R a_a19Hg)
arimaTheta :: forall a_a19l6 b_a19l7 b_a19Hh. Lens (ARIMAp a_a19l6 b_a19l7) (ARIMAp a_a19l6 b_a19Hh) (R b_a19l7) (R b_a19Hh)
arimaConstant :: forall a_a19l6 b_a19l7. Lens' (ARIMAp a_a19l6 b_a19l7) Double
arimaYPred :: forall a_a19Ia b_a19Ib c_a19Ic. Lens' (ARIMAs a_a19Ia b_a19Ib c_a19Ic) Double
arimaYHist :: forall a_a19Ia b_a19Ib c_a19Ic a_a19ON b_a19OO. Lens (ARIMAs a_a19Ia b_a19Ib c_a19Ic) (ARIMAs a_a19ON b_a19OO c_a19Ic) (R ((+) a_a19Ia b_a19Ib)) (R ((+) a_a19ON b_a19OO))
arimaEHist :: forall a_a19Ia b_a19Ib c_a19Ic c_a19OM. Lens (ARIMAs a_a19Ia b_a19Ib c_a19Ic) (ARIMAs a_a19Ia b_a19Ib c_a19OM) (R c_a19Ic) (R c_a19OM)
instance Control.DeepSeq.NFData (Backprop.Learn.Model.Regression.ARIMAp p q)
instance Control.DeepSeq.NFData (Backprop.Learn.Model.Regression.ARIMAs p d q)
instance GHC.Num.Num (Backprop.Learn.Model.Regression.ARIMAp p q)
instance GHC.Num.Num (Backprop.Learn.Model.Regression.ARIMAs p d q)
instance GHC.Real.Fractional (Backprop.Learn.Model.Regression.ARIMAp p q)
instance GHC.Real.Fractional (Backprop.Learn.Model.Regression.ARIMAs p d q)
instance GHC.Float.Floating (Backprop.Learn.Model.Regression.ARIMAp p q)
instance GHC.Float.Floating (Backprop.Learn.Model.Regression.ARIMAs p d q)
instance Numeric.Opto.Update.Additive (Backprop.Learn.Model.Regression.ARIMAp p q)
instance Numeric.Opto.Update.Additive (Backprop.Learn.Model.Regression.ARIMAs p d q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat q) => Numeric.Opto.Update.Scaling GHC.Types.Double (Backprop.Learn.Model.Regression.ARIMAp p q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat d, GHC.TypeNats.KnownNat q) => Numeric.Opto.Update.Scaling GHC.Types.Double (Backprop.Learn.Model.Regression.ARIMAs p d q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat q) => Numeric.Opto.Update.Metric GHC.Types.Double (Backprop.Learn.Model.Regression.ARIMAp p q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat d, GHC.TypeNats.KnownNat q) => Numeric.Opto.Update.Metric GHC.Types.Double (Backprop.Learn.Model.Regression.ARIMAs p d q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat q, Numeric.Opto.Ref.Ref m (Backprop.Learn.Model.Regression.ARIMAp p q) v) => Numeric.Opto.Update.AdditiveInPlace m v (Backprop.Learn.Model.Regression.ARIMAp p q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat d, GHC.TypeNats.KnownNat q, Numeric.Opto.Ref.Ref m (Backprop.Learn.Model.Regression.ARIMAs p d q) v) => Numeric.Opto.Update.AdditiveInPlace m v (Backprop.Learn.Model.Regression.ARIMAs p d q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat q, Numeric.Opto.Ref.Ref m (Backprop.Learn.Model.Regression.ARIMAp p q) v) => Numeric.Opto.Update.ScalingInPlace m v GHC.Types.Double (Backprop.Learn.Model.Regression.ARIMAp p q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat d, GHC.TypeNats.KnownNat q, Numeric.Opto.Ref.Ref m (Backprop.Learn.Model.Regression.ARIMAs p d q) v) => Numeric.Opto.Update.ScalingInPlace m v GHC.Types.Double (Backprop.Learn.Model.Regression.ARIMAs p d q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat q) => Backprop.Learn.Initialize.Initialize (Backprop.Learn.Model.Regression.ARIMAp p q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat d, GHC.TypeNats.KnownNat q) => Backprop.Learn.Initialize.Initialize (Backprop.Learn.Model.Regression.ARIMAs p d q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat q) => Data.Binary.Class.Binary (Backprop.Learn.Model.Regression.ARIMAp p q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat d, GHC.TypeNats.KnownNat q) => Data.Binary.Class.Binary (Backprop.Learn.Model.Regression.ARIMAs p d q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat q) => Numeric.Backprop.Class.Backprop (Backprop.Learn.Model.Regression.ARIMAp p q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat d, GHC.TypeNats.KnownNat q) => Numeric.Backprop.Class.Backprop (Backprop.Learn.Model.Regression.ARIMAs p d q)
instance (GHC.TypeNats.KnownNat a, GHC.TypeNats.KnownNat b, GHC.TypeNats.KnownNat c) => GHC.Show.Show (Backprop.Learn.Model.Regression.ARIMAs a b c)
instance GHC.Generics.Generic (Backprop.Learn.Model.Regression.ARIMAs a b c)
instance (GHC.TypeNats.KnownNat a, GHC.TypeNats.KnownNat b) => GHC.Show.Show (Backprop.Learn.Model.Regression.ARIMAp a b)
instance GHC.Generics.Generic (Backprop.Learn.Model.Regression.ARIMAp a b)
instance Control.DeepSeq.NFData (Backprop.Learn.Model.Regression.LRp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Backprop.Learn.Initialize.Initialize (Backprop.Learn.Model.Regression.LRp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Numeric.Opto.Update.Additive (Backprop.Learn.Model.Regression.LRp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Numeric.Opto.Update.Scaling GHC.Types.Double (Backprop.Learn.Model.Regression.LRp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Numeric.Opto.Update.Metric GHC.Types.Double (Backprop.Learn.Model.Regression.LRp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o, Numeric.Opto.Ref.Ref m (Backprop.Learn.Model.Regression.LRp i o) v) => Numeric.Opto.Update.AdditiveInPlace m v (Backprop.Learn.Model.Regression.LRp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o, Numeric.Opto.Ref.Ref m (Backprop.Learn.Model.Regression.LRp i o) v) => Numeric.Opto.Update.ScalingInPlace m v GHC.Types.Double (Backprop.Learn.Model.Regression.LRp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Data.Binary.Class.Binary (Backprop.Learn.Model.Regression.LRp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Numeric.Backprop.Class.Backprop (Backprop.Learn.Model.Regression.LRp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => GHC.Num.Num (Backprop.Learn.Model.Regression.LRp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => GHC.Real.Fractional (Backprop.Learn.Model.Regression.LRp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => GHC.Float.Floating (Backprop.Learn.Model.Regression.LRp i o)
instance (GHC.TypeNats.KnownNat o, GHC.TypeNats.KnownNat i) => GHC.Show.Show (Backprop.Learn.Model.Regression.LRp i o)
instance GHC.Generics.Generic (Backprop.Learn.Model.Regression.LRp i o)

module Backprop.Learn.Model.Neural

-- | Parameters for fully connected feed-forward layer with bias.
type FCp = LRp

-- | Fully connected feed-forward layer with bias. Parameterized by its
--   initialization distribution.
--   
--   Note that this has no activation function; to use as a model with
--   activation function, chain it with an activation function using
--   <tt>RMap</tt>, <tt>:.~</tt>, etc.; see <tt>FCA</tt> for a convenient
--   type synonym and constructor.
--   
--   Without any activation function, this is essentially a multivariate
--   linear regression.
--   
--   With the logistic function as an activation function, this is
--   essentially multivariate logistic regression. (See <a>logReg</a>)
fc :: (KnownNat i, KnownNat o) => Model ( 'Just (FCp i o))  'Nothing (R i) (R o)

-- | Convenient synonym for an <tt>fC</tt> post-composed with a simple
--   parameterless activation function.
fca :: (KnownNat i, KnownNat o) => (forall z. Reifies z W => BVar z (R o) -> BVar z (R o)) -> Model ( 'Just (FCp i o))  'Nothing (R i) (R o)
fcWeights :: Lens (FCp i o) (FCp i' o) (L o i) (L o i')
fcBias :: Lens' (FCp i o) (R o)

-- | Fully connected recurrent layer with bias.
fcr :: (KnownNat i, KnownNat o, KnownNat s) => (forall z. Reifies z W => BVar z (R o) -> BVar z (R s)) -> Model ( 'Just (FCRp s i o)) ( 'Just (R s)) (R i) (R o)

-- | Convenient synonym for an <a>fcr</a> post-composed with a simple
--   parameterless activation function.
fcra :: (KnownNat i, KnownNat o, KnownNat s) => (forall z. Reifies z W => BVar z (R o) -> BVar z (R o)) -> (forall z. Reifies z W => BVar z (R o) -> BVar z (R s)) -> Model ( 'Just (FCRp s i o)) ( 'Just (R s)) (R i) (R o)

-- | Parameter for fully connected recurrent layer.
type FCRp s i o = FCp (i + s) o
fcrBias :: Lens' (FCRp s i o) (R o)
fcrInputWeights :: (KnownNat s, KnownNat i, KnownNat i', KnownNat o) => Lens (FCRp s i o) (FCRp s i' o) (L o i) (L o i')
fcrStateWeights :: (KnownNat s, KnownNat s', KnownNat i, KnownNat o) => Lens (FCRp s i o) (FCRp s' i o) (L o s) (L o s')

module Backprop.Learn.Model.Neural.LSTM

-- | Long-term short-term memory layer
--   
--   <a>http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>
lstm :: (KnownNat i, KnownNat o) => Model ( 'Just (LSTMp i o)) ( 'Just (T2 (R o) (R o))) (R i) (R o)

-- | <tt>LSTM</tt> layer parmateters
data LSTMp (i :: Nat) (o :: Nat)
LSTMp :: !(FCp (i + o) o) -> !(FCp (i + o) o) -> !(FCp (i + o) o) -> !(FCp (i + o) o) -> LSTMp
[_lstmForget] :: LSTMp -> !(FCp (i + o) o)
[_lstmInput] :: LSTMp -> !(FCp (i + o) o)
[_lstmUpdate] :: LSTMp -> !(FCp (i + o) o)
[_lstmOutput] :: LSTMp -> !(FCp (i + o) o)
lstmForget :: forall i_a1zYE o_a1zYF. Lens' (LSTMp i_a1zYE o_a1zYF) (FCp ((+) i_a1zYE o_a1zYF) o_a1zYF)
lstmInput :: forall i_a1zYE o_a1zYF. Lens' (LSTMp i_a1zYE o_a1zYF) (FCp ((+) i_a1zYE o_a1zYF) o_a1zYF)
lstmUpdate :: forall i_a1zYE o_a1zYF. Lens' (LSTMp i_a1zYE o_a1zYF) (FCp ((+) i_a1zYE o_a1zYF) o_a1zYF)
lstmOutput :: forall i_a1zYE o_a1zYF. Lens' (LSTMp i_a1zYE o_a1zYF) (FCp ((+) i_a1zYE o_a1zYF) o_a1zYF)

-- | Gated Recurrent Unit
--   
--   <a>http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>
gru :: (KnownNat i, KnownNat o) => Model ( 'Just (GRUp i o)) ( 'Just (R o)) (R i) (R o)

-- | <tt>GRU</tt> layer parmateters
data GRUp (i :: Nat) (o :: Nat)
GRUp :: !(FCp (i + o) o) -> !(FCp (i + o) o) -> !(FCp (i + o) o) -> GRUp
[_gruMemory] :: GRUp -> !(FCp (i + o) o)
[_gruUpdate] :: GRUp -> !(FCp (i + o) o)
[_gruOutput] :: GRUp -> !(FCp (i + o) o)
gruMemory :: forall i_a1A5a o_a1A5b. Lens' (GRUp i_a1A5a o_a1A5b) (FCp ((+) i_a1A5a o_a1A5b) o_a1A5b)
gruUpdate :: forall i_a1A5a o_a1A5b. Lens' (GRUp i_a1A5a o_a1A5b) (FCp ((+) i_a1A5a o_a1A5b) o_a1A5b)
gruOutput :: forall i_a1A5a o_a1A5b. Lens' (GRUp i_a1A5a o_a1A5b) (FCp ((+) i_a1A5a o_a1A5b) o_a1A5b)
instance Control.DeepSeq.NFData (Backprop.Learn.Model.Neural.LSTM.GRUp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Numeric.Opto.Update.Additive (Backprop.Learn.Model.Neural.LSTM.GRUp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Numeric.Opto.Update.Scaling GHC.Types.Double (Backprop.Learn.Model.Neural.LSTM.GRUp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Numeric.Opto.Update.Metric GHC.Types.Double (Backprop.Learn.Model.Neural.LSTM.GRUp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o, Numeric.Opto.Ref.Ref m (Backprop.Learn.Model.Neural.LSTM.GRUp i o) v) => Numeric.Opto.Update.AdditiveInPlace m v (Backprop.Learn.Model.Neural.LSTM.GRUp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o, Numeric.Opto.Ref.Ref m (Backprop.Learn.Model.Neural.LSTM.GRUp i o) v) => Numeric.Opto.Update.ScalingInPlace m v GHC.Types.Double (Backprop.Learn.Model.Neural.LSTM.GRUp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Data.Binary.Class.Binary (Backprop.Learn.Model.Neural.LSTM.GRUp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Numeric.Backprop.Class.Backprop (Backprop.Learn.Model.Neural.LSTM.GRUp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Backprop.Learn.Initialize.Initialize (Backprop.Learn.Model.Neural.LSTM.GRUp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => GHC.Num.Num (Backprop.Learn.Model.Neural.LSTM.GRUp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => GHC.Real.Fractional (Backprop.Learn.Model.Neural.LSTM.GRUp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => GHC.Float.Floating (Backprop.Learn.Model.Neural.LSTM.GRUp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => GHC.Show.Show (Backprop.Learn.Model.Neural.LSTM.GRUp i o)
instance GHC.Generics.Generic (Backprop.Learn.Model.Neural.LSTM.GRUp i o)
instance Control.DeepSeq.NFData (Backprop.Learn.Model.Neural.LSTM.LSTMp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Numeric.Opto.Update.Additive (Backprop.Learn.Model.Neural.LSTM.LSTMp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Numeric.Opto.Update.Scaling GHC.Types.Double (Backprop.Learn.Model.Neural.LSTM.LSTMp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Numeric.Opto.Update.Metric GHC.Types.Double (Backprop.Learn.Model.Neural.LSTM.LSTMp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o, Numeric.Opto.Ref.Ref m (Backprop.Learn.Model.Neural.LSTM.LSTMp i o) v) => Numeric.Opto.Update.AdditiveInPlace m v (Backprop.Learn.Model.Neural.LSTM.LSTMp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o, Numeric.Opto.Ref.Ref m (Backprop.Learn.Model.Neural.LSTM.LSTMp i o) v) => Numeric.Opto.Update.ScalingInPlace m v GHC.Types.Double (Backprop.Learn.Model.Neural.LSTM.LSTMp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Data.Binary.Class.Binary (Backprop.Learn.Model.Neural.LSTM.LSTMp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Numeric.Backprop.Class.Backprop (Backprop.Learn.Model.Neural.LSTM.LSTMp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Backprop.Learn.Initialize.Initialize (Backprop.Learn.Model.Neural.LSTM.LSTMp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => GHC.Num.Num (Backprop.Learn.Model.Neural.LSTM.LSTMp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => GHC.Real.Fractional (Backprop.Learn.Model.Neural.LSTM.LSTMp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => GHC.Float.Floating (Backprop.Learn.Model.Neural.LSTM.LSTMp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => GHC.Show.Show (Backprop.Learn.Model.Neural.LSTM.LSTMp i o)
instance GHC.Generics.Generic (Backprop.Learn.Model.Neural.LSTM.LSTMp i o)

module Backprop.Learn.Model

-- | Class of values that can be backpropagated in general.
--   
--   For instances of <a>Num</a>, these methods can be given by
--   <a>zeroNum</a>, <a>addNum</a>, and <a>oneNum</a>. There are also
--   generic options given in <a>Numeric.Backprop.Class</a> for functors,
--   <a>IsList</a> instances, and <a>Generic</a> instances.
--   
--   <pre>
--   instance <a>Backprop</a> <a>Double</a> where
--       <a>zero</a> = <a>zeroNum</a>
--       <a>add</a> = <a>addNum</a>
--       <a>one</a> = <a>oneNum</a>
--   </pre>
--   
--   If you leave the body of an instance declaration blank, GHC Generics
--   will be used to derive instances if the type has a single constructor
--   and each field is an instance of <a>Backprop</a>.
--   
--   To ensure that backpropagation works in a sound way, should obey the
--   laws:
--   
--   <ul>
--   <li><i><i>identity</i></i></li>
--   </ul>
--   
--   <ul>
--   <li><pre><a>add</a> x (<a>zero</a> y) = x</pre></li>
--   <li><pre><a>add</a> (<a>zero</a> x) y = y</pre></li>
--   </ul>
--   
--   Also implies preservation of information, making <tt><a>zipWith</a>
--   (<a>+</a>)</tt> an illegal implementation for lists and vectors.
--   
--   This is only expected to be true up to potential "extra zeroes" in
--   <tt>x</tt> and <tt>y</tt> in the result.
--   
--   <ul>
--   <li><i><i>commutativity</i></i></li>
--   </ul>
--   
--   <ul>
--   <li><pre><a>add</a> x y = <a>add</a> y x</pre></li>
--   </ul>
--   
--   <ul>
--   <li><i><i>associativity</i></i></li>
--   </ul>
--   
--   <ul>
--   <li><pre><a>add</a> x (<a>add</a> y z) = <a>add</a> (<a>add</a> x y)
--   z</pre></li>
--   </ul>
--   
--   <ul>
--   <li><i><i>idempotence</i></i></li>
--   </ul>
--   
--   <ul>
--   <li><pre><a>zero</a> <a>.</a> <a>zero</a> = <a>zero</a></pre></li>
--   <li><pre><a>one</a> <a>.</a> <a>one</a> = <a>one</a></pre></li>
--   </ul>
--   
--   Note that not all values in the backpropagation process needs all of
--   these methods: Only the "final result" needs <a>one</a>, for example.
--   These are all grouped under one typeclass for convenience in defining
--   instances, and also to talk about sensible laws. For fine-grained
--   control, use the "explicit" versions of library functions (for
--   example, in <a>Numeric.Backprop.Explicit</a>) instead of
--   <a>Backprop</a> based ones.
--   
--   This typeclass replaces the reliance on <a>Num</a> of the previous API
--   (v0.1). <a>Num</a> is strictly more powerful than <a>Backprop</a>, and
--   is a stronger constraint on types than is necessary for proper
--   backpropagating. In particular, <a>fromInteger</a> is a problem for
--   many types, preventing useful backpropagation for lists,
--   variable-length vectors (like <a>Data.Vector</a>) and variable-size
--   matrices from linear algebra libraries like <i>hmatrix</i> and
--   <i>accelerate</i>.
class Backprop a

-- | "Zero out" all components of a value. For scalar values, this should
--   just be <tt><a>const</a> 0</tt>. For vectors and matrices, this should
--   set all components to zero, the additive identity.
--   
--   Should be idempotent:
--   
--   <ul>
--   <li><pre><a>zero</a> <a>.</a> <a>zero</a> = <a>zero</a></pre></li>
--   </ul>
--   
--   Should be as <i>lazy</i> as possible. This behavior is observed for
--   all instances provided by this library.
--   
--   See <a>zeroNum</a> for a pre-built definition for instances of
--   <a>Num</a> and <a>zeroFunctor</a> for a definition for instances of
--   <a>Functor</a>. If left blank, will automatically be
--   <a>genericZero</a>, a pre-built definition for instances of
--   <a>Generic</a> whose fields are all themselves instances of
--   <a>Backprop</a>.
zero :: Backprop a => a -> a

-- | Add together two values of a type. To combine contributions of
--   gradients, so should be information-preserving:
--   
--   <ul>
--   <li><pre><a>add</a> x (<a>zero</a> y) = x</pre></li>
--   <li><pre><a>add</a> (<a>zero</a> x) y = y</pre></li>
--   </ul>
--   
--   Should be as <i>strict</i> as possible. This behavior is observed for
--   all instances provided by this library.
--   
--   See <a>addNum</a> for a pre-built definition for instances of
--   <a>Num</a> and <tt>addFunctor</tt> for a definition for instances of
--   <a>Functor</a>. If left blank, will automatically be
--   <a>genericAdd</a>, a pre-built definition for instances of
--   <a>Generic</a> with one constructor whose fields are all themselves
--   instances of <a>Backprop</a>.
add :: Backprop a => a -> a -> a

-- | <a>One</a> all components of a value. For scalar values, this should
--   just be <tt><a>const</a> 1</tt>. For vectors and matrices, this should
--   set all components to one, the multiplicative identity.
--   
--   Should be idempotent:
--   
--   <ul>
--   <li><pre><a>one</a> <a>.</a> <a>one</a> = <a>one</a></pre></li>
--   </ul>
--   
--   Should be as <i>lazy</i> as possible. This behavior is observed for
--   all instances provided by this library.
--   
--   See <a>oneNum</a> for a pre-built definition for instances of
--   <a>Num</a> and <a>oneFunctor</a> for a definition for instances of
--   <a>Functor</a>. If left blank, will automatically be
--   <a>genericOne</a>, a pre-built definition for instances of
--   <a>Generic</a> whose fields are all themselves instances of
--   <a>Backprop</a>.
one :: Backprop a => a -> a
runModel :: forall p s a b. (MaybeC Backprop s, Backprop b) => Model p s a b -> Mayb I p -> a -> Mayb I s -> (b, Mayb I s)
runModelStoch :: forall p s a b m. (MaybeC Backprop s, Backprop b, PrimMonad m) => Model p s a b -> Gen (PrimState m) -> Mayb I p -> a -> Mayb I s -> m (b, Mayb I s)
runModelStateless :: Model p  'Nothing a b -> Mayb I p -> a -> b
runModelStochStateless :: PrimMonad m => Model p  'Nothing a b -> Gen (PrimState m) -> Mayb I p -> a -> m b
gradModel :: (Backprop a, Backprop b, MaybeC Backprop p) => Model p  'Nothing a b -> Mayb I p -> a -> (Mayb I p, a)
gradModelStoch :: (Backprop a, Backprop b, MaybeC Backprop p, PrimMonad m) => Model p  'Nothing a b -> Gen (PrimState m) -> Mayb I p -> a -> m (Mayb I p, a)
initParam :: (Initialize p, ContGen d, PrimMonad m) => model ( 'Just p) s a b -> d -> Gen (PrimState m) -> m p
initParamNormal :: (Initialize p, PrimMonad m) => model ( 'Just p) s a b -> Double -> Gen (PrimState m) -> m p
encodeParam :: Binary p => model ( 'Just p) s a b -> p -> ByteString
decodeParam :: Binary p => model ( 'Just p) s a b -> ByteString -> p
decodeParamOrFail :: Binary p => model ( 'Just p) s a b -> ByteString -> Either String p
saveParam :: Binary p => model ( 'Just p) s a b -> FilePath -> p -> IO ()
loadParam :: Binary p => model ( 'Just p) s a b -> FilePath -> IO p
loadParamOrFail :: Binary p => model ( 'Just p) s a b -> FilePath -> IO (Either String p)
iterateModel :: (Backprop b, MaybeC Backprop s) => (b -> a) -> Int -> Model p s a b -> Mayb I p -> a -> Mayb I s -> ([b], Mayb I s)
iterateModelM :: (Backprop b, MaybeC Backprop s, Monad m) => (b -> m a) -> Int -> Model p s a b -> Mayb I p -> a -> Mayb I s -> m ([b], Mayb I s)
iterateModelStoch :: (Backprop b, MaybeC Backprop s, PrimMonad m) => (b -> m a) -> Int -> Model p s a b -> Gen (PrimState m) -> Mayb I p -> a -> Mayb I s -> m ([b], Mayb I s)
scanModel :: (Traversable t, Backprop b, MaybeC Backprop s) => Model p s a b -> Mayb I p -> t a -> Mayb I s -> (t b, Mayb I s)
scanModelStoch :: (Traversable t, Backprop b, MaybeC Backprop s, PrimMonad m) => Model p s a b -> Gen (PrimState m) -> Mayb I p -> t a -> Mayb I s -> m (t b, Mayb I s)
iterateModel_ :: (Backprop b, MaybeC Backprop s) => (b -> a) -> Model p s a b -> Mayb I p -> a -> Mayb I s -> [b]
iterateModelM_ :: (Backprop b, MaybeC Backprop s, Monad m) => (b -> m a) -> Int -> Model p s a b -> Mayb I p -> a -> Mayb I s -> m [b]
iterateModelStoch_ :: (Backprop b, MaybeC Backprop s, PrimMonad m) => (b -> m a) -> Int -> Model p s a b -> Gen (PrimState m) -> Mayb I p -> a -> Mayb I s -> m [b]
scanModel_ :: (Traversable t, Backprop b, MaybeC Backprop s) => Model p s a b -> Mayb I p -> t a -> Mayb I s -> t b
scanModelStoch_ :: (Traversable t, Backprop b, MaybeC Backprop s, PrimMonad m) => Model p s a b -> Gen (PrimState m) -> Mayb I p -> t a -> Mayb I s -> m (t b)
primeModel :: (Foldable t, Backprop b, MaybeC Backprop s) => Model p s a b -> Mayb I p -> t a -> Mayb I s -> Mayb I s
primeModelStoch :: (Foldable t, Backprop b, MaybeC Backprop s, PrimMonad m) => Model p s a b -> Gen (PrimState m) -> Mayb I p -> t a -> Mayb I s -> m (Mayb I s)
selfPrime :: (Backprop b, MaybeC Backprop s) => (b -> a) -> Model p s a b -> Mayb I p -> a -> Mayb I s -> [Mayb I s]
selfPrimeM :: (Backprop b, MaybeC Backprop s, Monad m) => (b -> m a) -> Int -> Model p s a b -> Mayb I p -> a -> Mayb I s -> m (Mayb I s)

module Backprop.Learn.Train

-- | Gradient of model with respect to loss function and target
gradModelLoss :: Backprop p => Loss b -> Regularizer p -> Model ( 'Just p)  'Nothing a b -> p -> a -> b -> p

-- | Stochastic gradient of model with respect to loss function and target
gradModelStochLoss :: (Backprop p, PrimMonad m) => Loss b -> Regularizer p -> Model ( 'Just p)  'Nothing a b -> Gen (PrimState m) -> p -> a -> b -> m p
type Grad (m :: * -> *) a = a -> m Diff a

-- | Using a model's deterministic prediction function (with a given loss
--   function), generate a <a>Grad</a> compatible with <a>Numeric.Opto</a>
--   and <a>Numeric.Opto.Run</a>.
modelGrad :: (MonadSample (a, b) m, Backprop p) => Loss b -> Regularizer p -> Model ( 'Just p)  'Nothing a b -> Grad m p

-- | Using a model's stochastic prediction function (with a given loss
--   function), generate a <a>Grad</a> compatible with <a>Numeric.Opto</a>
--   and <a>Numeric.Opto.Run</a>.
modelGradStoch :: (MonadSample (a, b) m, PrimMonad m, Backprop p) => Loss b -> Regularizer p -> Model ( 'Just p)  'Nothing a b -> Gen (PrimState m) -> Grad m p

module Backprop.Learn.Test
type Test o = o -> o -> Double
maxIxTest :: KnownNat n => Test (R n)
rmseTest :: forall n. KnownNat n => Test (R n)
squaredErrorTest :: Real a => Test a
absErrorTest :: Real a => Test a
totalSquaredErrorTest :: (Applicative t, Foldable t, Real a) => Test (t a)
squaredErrorTestV :: KnownNat n => Test (R n)
crossEntropyTest :: KnownNat n => Test (R n)

-- | Create a <a>Test</a> from a <a>Loss</a>
lossTest :: Loss a -> Test a
lmapTest :: (a -> b) -> Test b -> Test a
testModel :: Test b -> Model p  'Nothing a b -> Mayb I p -> a -> b -> Double
testModelStoch :: PrimMonad m => Test b -> Model p  'Nothing a b -> Gen (PrimState m) -> Mayb I p -> a -> b -> m Double
testModelAll :: Foldable t => Test b -> Model p  'Nothing a b -> Mayb I p -> t (a, b) -> Double
testModelStochAll :: (Foldable t, PrimMonad m) => Test b -> Model p  'Nothing a b -> Gen (PrimState m) -> Mayb I p -> t (a, b) -> m Double
testModelCov :: (Foldable t, Fractional b) => Model p  'Nothing a b -> Mayb I p -> t (a, b) -> b
testModelCorr :: (Foldable t, Floating b) => Model p  'Nothing a b -> Mayb I p -> t (a, b) -> b
testModelStochCov :: (Foldable t, PrimMonad m, Fractional b) => Model p  'Nothing a b -> Gen (PrimState m) -> Mayb I p -> t (a, b) -> m b
testModelStochCorr :: (Foldable t, PrimMonad m, Floating b) => Model p  'Nothing a b -> Gen (PrimState m) -> Mayb I p -> t (a, b) -> m b

module Backprop.Learn.Run
consecutives :: Monad m => ConduitT i (i, i) m ()
consecutivesN :: forall v n i m. (KnownNat n, Vector v i, Monad m) => ConduitT i (Vector v n i, Vector v n i) m ()
leadings :: forall v n i m. (KnownNat n, Vector v i, Monad m) => ConduitT i (Vector v n i, i) m ()
conduitModel :: (Backprop b, MaybeC Backprop s, Monad m) => Model p s a b -> Mayb I p -> Mayb I s -> ConduitT a b m (Mayb I s)
conduitModelStoch :: (Backprop b, MaybeC Backprop s, PrimMonad m) => Model p s a b -> Gen (PrimState m) -> Mayb I p -> Mayb I s -> ConduitT a b m (Mayb I s)

-- | What module should this be in?
oneHot' :: (Vector v a, KnownNat n) => a -> a -> Finite n -> Vector v n a
oneHot :: (Vector v a, KnownNat n, Num a) => Finite n -> Vector v n a
oneHotR :: KnownNat n => Finite n -> R n

-- | <i>O(n)</i> Yield the index of the maximum element of the non-empty
--   vector.
maxIndex :: (Vector v a, Ord a, KnownNat n) => Vector v n + 1 a -> Finite n + 1

-- | Could be in <i>hmatrix</i>.
maxIndexR :: KnownNat n => R (n + 1) -> Finite (n + 1)

module Backprop.Learn
