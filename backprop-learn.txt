-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Combinators and useful tools for ANNs using the backprop library
--   
--   See README.md
@package backprop-learn
@version 0.1.0.0

module Backprop.Learn.Loss
type Loss a = forall s. Reifies s W => a -> BVar s a -> BVar s Double
crossEntropy :: KnownNat n => Loss (R n)
squaredError :: Loss Double
totalSquaredError :: (Num (t Double), Foldable t, Functor t) => Loss (t Double)
squaredErrorV :: KnownNat n => Loss (R n)
sumLoss :: (Traversable t, Applicative t, Num a) => Loss a -> Loss (t a)
sumLossDecay :: forall n a. (KnownNat n, Num a) => Double -> Loss a -> Loss (Vector n a)
lastLoss :: (KnownNat (n + 1), Num a) => Loss a -> Loss (Vector (n + 1) a)
zipLoss :: (Traversable t, Applicative t, Num a) => t Double -> Loss a -> Loss (t a)

module Data.Type.Mayb
data Mayb :: (k -> Type) -> Maybe k -> Type
[N_] :: Mayb f  'Nothing
[J_] :: !(f a) -> Mayb f ( 'Just a)
fromJ_ :: Mayb f ( 'Just a) -> f a
maybToList :: Mayb f m -> Prod f (MaybeToList m)
listToMayb :: Prod f as -> Mayb f (ListToMaybe as)
data P :: k -> Type
[P] :: P a
type KnownMayb = Known (Mayb P)
knownMayb :: KnownMayb p => Mayb P p
zipMayb :: (forall a. f a -> g a -> h a) -> Mayb f m -> Mayb g m -> Mayb h m
zipMayb3 :: (forall a. f a -> g a -> h a -> i a) -> Mayb f m -> Mayb g m -> Mayb h m -> Mayb i m
class MaybeWit (c :: k -> Constraint) (m :: Maybe k)
maybeWit :: MaybeWit c m => Mayb (Wit1 c) m
splitTupMaybe :: forall f a b. (KnownMayb a, KnownMayb b) => (forall a' b'. (a ~  'Just a', b ~  'Just b') => f (T2 a' b') -> (f a', f b')) -> Mayb f (TupMaybe a b) -> (Mayb f a, Mayb f b)
tupMaybe :: forall f a b. () => (forall a' b'. (a ~  'Just a', b ~  'Just b') => f a' -> f b' -> f (T2 a' b')) -> Mayb f a -> Mayb f b -> Mayb f (TupMaybe a b)
instance forall k (f :: k -> *) (m :: GHC.Base.Maybe k). Data.Type.Mayb.MaybeC GHC.Show.Show (f Data.Type.Mayb.<$> m) => GHC.Show.Show (Data.Type.Mayb.Mayb f m)
instance forall k (c :: k -> GHC.Types.Constraint) (m :: GHC.Base.Maybe k). (Data.Type.Mayb.MaybeC c m, Type.Class.Known.Known (Data.Type.Mayb.Mayb Data.Type.Mayb.P) m) => Data.Type.Mayb.MaybeWit c m
instance forall k1 (k2 :: k1). Type.Class.Known.Known Data.Type.Mayb.P k2
instance forall k (m :: GHC.Base.Maybe k) (f :: k -> *). (Type.Class.Known.Known (Data.Type.Mayb.Mayb Data.Type.Mayb.P) m, Data.Type.Mayb.MaybeC GHC.Num.Num (f Data.Type.Mayb.<$> m)) => GHC.Num.Num (Data.Type.Mayb.Mayb f m)
instance forall k (m :: GHC.Base.Maybe k) (f :: k -> *). (Type.Class.Known.Known (Data.Type.Mayb.Mayb Data.Type.Mayb.P) m, Data.Type.Mayb.MaybeC GHC.Num.Num (f Data.Type.Mayb.<$> m), Data.Type.Mayb.MaybeC GHC.Real.Fractional (f Data.Type.Mayb.<$> m)) => GHC.Real.Fractional (Data.Type.Mayb.Mayb f m)
instance forall k (f :: k -> *). Type.Class.Known.Known (Data.Type.Mayb.Mayb f) 'GHC.Base.Nothing
instance forall a1 (f :: a1 -> *) (a2 :: a1). Type.Class.Known.Known f a2 => Type.Class.Known.Known (Data.Type.Mayb.Mayb f) ('GHC.Base.Just a2)
instance Type.Class.Higher.Functor1 Data.Type.Mayb.Mayb

module Backprop.Learn.Model.Class

-- | Class for models that can be trained using gradient descent
--   
--   An instance <tt>l</tt> of <tt><a>Learn</a> a b</tt> is parameterized
--   by <tt>p</tt>, takes <tt>a</tt> as input, and returns <tt>b</tt> as
--   outputs. <tt>l</tt> can be thought of as a value containing the
--   <i>hyperparmaeters</i> of the model.
class Learn a b l | l -> a b where {
    type family LParamMaybe l :: Maybe Type;
    type family LStateMaybe l :: Maybe Type;
    type LParamMaybe l =  'Nothing;
    type LStateMaybe l =  'Nothing;
}

-- | Initialize parameters, given the hyperparameters in <tt>l</tt>.
--   
--   Default definition provided for models with no state.
initParam :: (Learn a b l, PrimMonad m) => l -> Gen (PrimState m) -> LParam_ m l

-- | Initialize parameters, given the hyperparameters in <tt>l</tt>.
--   
--   Default definition provided for models with no state.
initParam :: (Learn a b l, NoParam l) => l -> Gen (PrimState m) -> LParam_ m l

-- | Initialize state, given the hyperparameters in <tt>l</tt>.
--   
--   Default definition provided for models with no state.
initState :: (Learn a b l, PrimMonad m) => l -> Gen (PrimState m) -> LState_ m l

-- | Initialize state, given the hyperparameters in <tt>l</tt>.
--   
--   Default definition provided for models with no state.
initState :: (Learn a b l, NoState l) => l -> Gen (PrimState m) -> LState_ m l

-- | Run the model itself, deterministically.
--   
--   If your model has no state, you can define this conveniently using
--   <a>stateless</a>.
runLearn :: (Learn a b l, Reifies s W) => l -> LParam_ (BVar s) l -> BVar s a -> LState_ (BVar s) l -> (BVar s b, LState_ (BVar s) l)

-- | Run a model in stochastic mode.
--   
--   If model is inherently non-stochastic, a default implementation is
--   given in terms of <a>runLearn</a>.
--   
--   If your model has no state, you can define this conveniently using
--   <tt>statelessStoch</tt>.
runLearnStoch :: (Learn a b l, Reifies s W, PrimMonad m) => l -> Gen (PrimState m) -> LParam_ (BVar s) l -> BVar s a -> LState_ (BVar s) l -> m (BVar s b, LState_ (BVar s) l)

-- | The trainable parameter type of a model. Will be a compile-time error
--   if the model has no trainable parameters.
type LParam l = FromJust ( 'ShowType l :<>:  'Text " has no trainable parameters") (LParamMaybe l)

-- | The state type of a model. Will be a compile-time error if the model
--   has no state.
type LState l = FromJust ( 'ShowType l :<>:  'Text " has no trainable parameters") (LStateMaybe l)

-- | Constraint specifying that a given model has no trainabale parameters.
type NoParam l = LParamMaybe l ~  'Nothing

-- | Constraint specifying that a given model has no state.
type NoState l = LStateMaybe l ~  'Nothing

-- | Is <a>N_</a> if there is <tt>l</tt> has no trainable parameters;
--   otherwise is <a>J_</a> with <tt>f p</tt>, for trainable parameter type
--   <tt>p</tt>.
type LParam_ f l = Mayb f (LParamMaybe l)

-- | Is <a>N_</a> if there is <tt>l</tt> has no state; otherwise is
--   <a>J_</a> with <tt>f s</tt>, for state type <tt>s</tt>.
type LState_ f l = Mayb f (LStateMaybe l)

-- | Useful for defining <a>runLearn</a> if your model has no state.
stateless :: (a -> b) -> (a -> s -> (b, s))

-- | Useful for defining <a>runLearnStoch</a> if your model has no state.
statelessM :: Functor m => (a -> m b) -> (a -> s -> m (b, s))
runLearnStateless :: (Learn a b l, Reifies s W, NoState l) => l -> LParam_ (BVar s) l -> BVar s a -> BVar s b
runLearnStochStateless :: (Learn a b l, Reifies s W, NoState l, PrimMonad m) => l -> Gen (PrimState m) -> LParam_ (BVar s) l -> BVar s a -> m (BVar s b)
data Mayb :: (k -> Type) -> Maybe k -> Type
[N_] :: Mayb f  'Nothing
[J_] :: !(f a) -> Mayb f ( 'Just a)
fromJ_ :: Mayb f ( 'Just a) -> f a
type KnownMayb = Known (Mayb P)
knownMayb :: KnownMayb p => Mayb P p
newtype I a
I :: a -> I a
[getI] :: I a -> a

module Backprop.Learn.Model.State

-- | Make a model stateless by converting the state to a trained parameter,
--   and dropping the modified state from the result.
--   
--   One of the ways to make a model stateless for training purposes.
--   Useful when used after <a>Unroll</a>. See <a>DeState</a>, as well.
--   
--   Its parameters are:
--   
--   <ul>
--   <li>If <tt>l</tt> has no parameters, just the initial state.</li>
--   <li>If <tt>l</tt> has parameters, a <a>T2</a> of the parameter and
--   initial state.</li>
--   </ul>
newtype TrainState :: Type -> Type
[TrainState] :: {getTrainState :: l} -> TrainState l

-- | Make a model stateless by pre-applying a fixed state (or a stochastic
--   one with fixed stribution) and dropping the modified state from the
--   result.
--   
--   One of the ways to make a model stateless for training purposes.
--   Useful when used after <a>Unroll</a>. See <a>TrainState</a>, as well.
data DeState :: Type -> Type -> Type
[DS] :: {_dsInitState :: s, _dsInitStateStoch :: forall m. PrimMonad m => Gen (PrimState m) -> m s, _dsLearn :: l} -> DeState s l

-- | Create a <a>DeState</a> from a deterministic, non-stochastic
--   initialization function.
dsDeterm :: s -> l -> DeState s l

-- | Unroll a (usually) stateful model into one taking a vector of
--   sequential inputs.
--   
--   Basically applies the model to every item of input and returns all of
--   the results, but propagating the state between every step.
--   
--   Useful when used before <a>TrainState</a> or <a>DeState</a>. See
--   <a>UnrollTrainState</a> and <a>UnrollDeState</a>.
--   
--   Compare to <tt>FeedbackTrace</tt>, which, instead of receiving a
--   vector of sequential inputs, receives a single input and uses its
--   output as the next input.
newtype Unroll :: Nat -> Type -> Type
[Unroll] :: {getUnroll :: l} -> Unroll n l

-- | Unroll a stateful model into a stateless one taking a vector of
--   sequential inputs and treat the initial state as a trained parameter.
--   
--   <pre>
--   instance <a>Learn</a> a b l
--       =&gt; <a>Learn</a> (<a>Vector</a> n a) (<a>Vector</a> n b) (<a>UnrollTrainState</a> n l)
--   
--       type <a>LParamMaybe</a> (<a>UnrollDeState</a> n l) = <a>TupMaybe</a> (<a>LParamMaybe</a> l) (<a>LStateMaybe</a> l)
--       type <a>LStateMaybe</a> (<a>UnrollDeState</a> n l) = '<a>Nothing</a>
--   </pre>
type UnrollTrainState n l = TrainState (Unroll n l)

-- | Constructor and pattern for <a>UnrollTrainState</a>
getUnrollTrainState :: () => UnrollTrainState n l -> l

-- | Unroll a stateful model into a stateless one taking a vector of
--   sequential inputs and fix the initial state.
--   
--   <pre>
--   instance <a>Learn</a> a b l
--       =&gt; <a>Learn</a> (<a>Vector</a> n a) (<a>Vector</a> n b) (<a>UnrollDeState</a> n l)
--   
--       type <a>LParamMaybe</a> (<a>UnrollDeState</a> n l) = <a>LParamMaybe</a> l
--       type <a>LStateMaybe</a> (<a>UnrollDeState</a> n l) = '<a>Nothing</a>
--   </pre>
type UnrollDeState n l = DeState (LState l) (Unroll n l)

-- | Constructor and pattern for <a>UnrollDeState</a>
_udsInitState :: () => UnrollDeState n l -> LState l
_udsInitStateStoch :: () => UnrollDeState n l -> forall (m :: * -> *). PrimMonad m => Gen PrimState m -> m LState l
_udsLearn :: () => UnrollDeState n l -> l

-- | Transform the state of a model by providing functions to pre-apply and
--   post-apply before and after the original model sees the state.
--   
--   I don't really know why you would ever want to do this, but it was fun
--   to write.
--   
--   <tt><a>ReState</a> p '()'</tt> is essentially <a>DeState</a>.
data ReState :: Type -> Type -> Type -> Type
[RS] :: {_rsInitState :: forall m. PrimMonad m => Gen (PrimState m) -> m t, _rsTo :: forall q. Reifies q W => BVar q s -> BVar q t, _rsFrom :: forall q. Reifies q W => BVar q t -> BVar q s, _rsFromStoch :: forall m q. (PrimMonad m, Reifies q W) => Gen (PrimState m) -> BVar q t -> m (BVar q s), _rsLearn :: l} -> ReState s t l

-- | Create a <a>ReState</a> from a deterministic, non-stochastic
--   transformation function.
rsDeterm :: (forall m. PrimMonad m => Gen (PrimState m) -> m t) -> (forall q. Reifies q W => BVar q s -> BVar q t) -> (forall q. Reifies q W => BVar q t -> BVar q s) -> l -> ReState s t l
instance (Backprop.Learn.Model.Class.Learn ab b l, Data.Type.Mayb.KnownMayb (Backprop.Learn.Model.Class.LStateMaybe l), GHC.Num.Num a, GHC.Num.Num b, GHC.Num.Num ab, Data.Type.Mayb.MaybeC GHC.Num.Num (Backprop.Learn.Model.Class.LStateMaybe l)) => Backprop.Learn.Model.Class.Learn a b (Backprop.Learn.Model.State.Recurrent ab a b l)
instance (Backprop.Learn.Model.Class.Learn a b l, Backprop.Learn.Model.Class.LStateMaybe l ~ 'GHC.Base.Just s) => Backprop.Learn.Model.Class.Learn a b (Backprop.Learn.Model.State.ReState s t l)
instance (Backprop.Learn.Model.Class.Learn a b l, Backprop.Learn.Model.Class.LStateMaybe l ~ 'GHC.Base.Just s) => Backprop.Learn.Model.Class.Learn a b (Backprop.Learn.Model.State.DeState s l)
instance (Backprop.Learn.Model.Class.Learn a b l, Data.Type.Mayb.KnownMayb (Backprop.Learn.Model.Class.LParamMaybe l), Data.Type.Mayb.MaybeC GHC.Num.Num (Backprop.Learn.Model.Class.LParamMaybe l), Backprop.Learn.Model.Class.LStateMaybe l ~ 'GHC.Base.Just s, GHC.Num.Num s) => Backprop.Learn.Model.Class.Learn a b (Backprop.Learn.Model.State.TrainState l)
instance (Backprop.Learn.Model.Class.Learn a b l, GHC.TypeNats.KnownNat n, GHC.Num.Num a, GHC.Num.Num b) => Backprop.Learn.Model.Class.Learn (Data.Vector.Sized.Vector n a) (Data.Vector.Sized.Vector n b) (Backprop.Learn.Model.State.Unroll n l)

module Backprop.Learn.Model.Parameter

-- | Convert a model with trainabile parameters into a model without any
--   trainable parameters.
--   
--   The parameters are instead fixed (or stochastic, with a fixed
--   distribution), and the model is treated as an untrainable function.
--   
--   <a>DeParam</a> is essentially <a>DeParamAt</a>, with <a>id</a> as the
--   lens.
data DeParam :: Type -> Type -> Type
[DP] :: {_dpParam :: p, _dpParamStoch :: forall m. PrimMonad m => Gen (PrimState m) -> m p, _dpLearn :: l} -> DeParam p l

-- | Create a <a>DeParam</a> from a deterministic, non-stochastic
--   parameter.
dpDeterm :: p -> l -> DeParam p l

-- | Wrapping a mode with <tt><a>DeParamAt</a> p q</tt> will fix a specific
--   part of the parameter type <tt>p</tt> to a given fixed (or stochastic
--   with a fixed distribution) value of type <tt>q</tt>. That field is not
--   backpropagated, and so its gradient will always be zero.
--   
--   The part is specified using a <a>Lens'</a>. This really only makes
--   sense if <tt>p</tt> is a record, and the lens points to a field (or
--   multiple fields, to a tuple) of the record.
--   
--   <a>DeParam</a> is essentially <a>DeParamAt</a>, with <a>id</a> as the
--   lens.
data DeParamAt :: Type -> Type -> Type -> Type
[DPA] :: {_dpaParam :: q, _dpaParamStoch :: forall m. PrimMonad m => Gen (PrimState m) -> m q, _dpaLens :: Lens' p q, _dpaLearn :: l} -> DeParamAt p q l

-- | Create a <a>DeParamAt</a> from a deterministic, non-stochastic fixed
--   value as a part of the parameter.
dpaDeterm :: q -> Lens' p q -> l -> DeParamAt p q l

-- | Pre-apply a function to the parameter before the original model sees
--   it. A <tt><a>ReParam</a> p q</tt> turns a model taking <tt>p</tt> into
--   a model taking <tt>q</tt>.
--   
--   Note that a <tt><a>ReParam</a> p '<a>Nothing</a></tt> is essentially
--   the same as a <tt><a>DeParam</a> p</tt>, and one could implement
--   <tt><a>DeParamAt</a> p q</tt> in terms of <tt><a>ReParam</a> p
--   ('<a>Just</a> q)</tt>.
data ReParam :: Type -> Maybe Type -> Type -> Type

-- | Create a <a>ReParam</a> from a deterministic, non-stochastic
--   transformation function.
rpDeterm :: (forall m. PrimMonad m => Gen (PrimState m) -> Mayb m q) -> (forall s. Reifies s W => Mayb (BVar s) q -> BVar s p) -> l -> ReParam p q l
instance (Backprop.Learn.Model.Class.Learn a b l, Backprop.Learn.Model.Class.LParamMaybe l ~ 'GHC.Base.Just p) => Backprop.Learn.Model.Class.Learn a b (Backprop.Learn.Model.Parameter.ReParam p q l)
instance (Backprop.Learn.Model.Class.Learn a b l, Backprop.Learn.Model.Class.LParamMaybe l ~ 'GHC.Base.Just p, GHC.Num.Num p, GHC.Num.Num q) => Backprop.Learn.Model.Class.Learn a b (Backprop.Learn.Model.Parameter.DeParamAt p q l)
instance (Backprop.Learn.Model.Class.Learn a b l, Backprop.Learn.Model.Class.LParamMaybe l ~ 'GHC.Base.Just p) => Backprop.Learn.Model.Class.Learn a b (Backprop.Learn.Model.Parameter.DeParam p l)

module Backprop.Learn.Model.Function

-- | A <tt><a>ParamFunc</a> p a b</tt> is a parameterized function from
--   <tt>a</tt> to <tt>b</tt>, potentially with trainable parameter
--   <tt>p</tt>.
--   
--   A utility wrapper for a deterministic and stateless model.
data ParamFunc p a b
PF :: forall m. PrimMonad m => Gen (PrimState m) -> Mayb m p -> forall s. Reifies s W => Mayb (BVar s) p -> BVar s a -> BVar s b -> ParamFunc p a b
[_pfInit] :: ParamFunc p a b -> forall m. PrimMonad m => Gen (PrimState m) -> Mayb m p
[_pfFunc] :: ParamFunc p a b -> forall s. Reifies s W => Mayb (BVar s) p -> BVar s a -> BVar s b

-- | Convenient type synonym for a <a>ParamFunc</a> with parameters.
--   
--   Mostly made to be easy to construct/deconstruct with <a>PFP</a>,
--   <a>_pfpInit</a>, and <a>_pfpFunc</a>.
type ParamFuncP p = ParamFunc ( 'Just p)
_pfpInit :: () => ParamFuncP p a b -> forall (m :: * -> *). PrimMonad m => Gen PrimState m -> m p
_pfpFunc :: () => ParamFuncP p a b -> forall s. Reifies s W => BVar s p -> BVar s a -> BVar s b

-- | An unparameterized function. Has a <a>Category</a> instance.
--   
--   A <tt><a>FixedFunc</a> a b</tt> essentially the same as a:
--   
--   <pre>
--   forall s. <a>Reifies</a> s <a>W</a> =&gt; <a>BVar</a> s a -&gt; <a>BVar</a> s b
--   </pre>
--   
--   And the <a>FF</a> pattern (and <a>runFixedFunc</a> extractor) witness
--   this.
--   
--   It is usually better to just use the functions directly, with
--   combinators like <tt>Dimap</tt>, <tt>LMap</tt>, <tt>RMap</tt>,
--   <a>dimapPF</a>, <a>lmapPF</a>, <a>rmapPF</a>, etc. This is just
--   provided to let you work nicely with <a>ParamFunc</a> combinators.
type FixedFunc = ParamFunc  'Nothing

-- | <a>FF</a> and <a>runFixedFunc</a> witness the fact that a
--   <tt><a>FixedFunc</a> a b</tt> is just a function from <tt><a>BVar</a>
--   s a</tt> to <tt><a>BVar</a> s b</tt>.
runFixedFunc :: () => FixedFunc a b -> forall s. Reifies s W => BVar s a -> BVar s b

-- | Utility function to make a <a>ParamFunc</a> that maps a parameterized
--   function over every item in the <a>R</a>. The parameter is shared
--   across the entire map, and trained.
paramMap :: KnownNat i => (forall m. PrimMonad m => Gen (PrimState m) -> Mayb m p) -> (forall s. Reifies s W => Mayb (BVar s) p -> BVar s Double -> BVar s Double) -> ParamFunc p (R i) (R i)

-- | Create a <a>ParamFunc</a> from any instance of <a>Learn</a> that does
--   not have state.
learnParam :: forall l a b. (Learn a b l, NoState l) => l -> ParamFunc (LParamMaybe l) a b

-- | Pre- and post-compose a <a>ParamFunc</a>
dimapPF :: (forall s. Reifies s W => BVar s a -> BVar s b) -> (forall s. Reifies s W => BVar s c -> BVar s d) -> ParamFunc p b c -> ParamFunc p a d

-- | Precompose a <a>ParamFunc</a>
lmapPF :: (forall s. Reifies s W => BVar s a -> BVar s b) -> ParamFunc p b c -> ParamFunc p a c

-- | Postcompose a <a>ParamFunc</a>
rmapPF :: (forall s. Reifies s W => BVar s b -> BVar s c) -> ParamFunc p a b -> ParamFunc p a c

-- | Compose two <a>ParamFunc</a>s sequentially
compPF :: forall p q a b c. (MaybeC Num p, MaybeC Num q, KnownMayb p, KnownMayb q) => ParamFunc p a b -> ParamFunc q b c -> ParamFunc (TupMaybe p q) a c

-- | Compose two <a>ParamFunc</a>s in parallel
parPF :: forall p q a b c d. (MaybeC Num p, MaybeC Num q, KnownMayb p, KnownMayb q, Num a, Num b, Num c, Num d) => ParamFunc p a c -> ParamFunc q b d -> ParamFunc (TupMaybe p q) (T2 a b) (T2 c d)

-- | Compose two <a>ParamFuncP</a>s on lists.
(.-) :: forall ps qs a b c. (ListC (Num <$> ps), ListC (Num <$> qs), Known Length ps, Known Length qs) => ParamFuncP (T ps) b c -> ParamFuncP (T qs) a b -> ParamFuncP (T (ps ++ qs)) a c
infixr 9 .-

-- | The identity of <a>.-</a>
nilPF :: ParamFuncP (T '[]) a a

-- | <a>ParamFuncP</a> taking a singleton list; meant to be used with
--   <a>.-</a>
onlyPF :: forall p a b. (KnownMayb p, MaybeC Num p) => ParamFunc p a b -> ParamFuncP (T (MaybeToList p)) a b

-- | Binary step / heaviside step
--   
--   To use with vectors (<a>R</a>), use <a>vmap'</a>.
--   
--   &lt;math&gt;
step :: (Ord a, Num a) => a -> a

-- | Logistic function
--   
--   &lt;math&gt;
logistic :: Floating a => a -> a

-- | Softsign activation function
--   
--   &lt;math&gt;
softsign :: Fractional a => a -> a

-- | Rectified linear unit.
--   
--   To use with vectors (<a>R</a>), use <a>vmap'</a>.
--   
--   &lt;math&gt;
--   
--   <pre>
--   <a>reLU</a> = <a>preLU</a> 0
--   </pre>
reLU :: (Num a, Ord a) => a -> a

-- | SoftPlus
--   
--   &lt;math&gt;
softPlus :: Floating a => a -> a

-- | Bent identity
--   
--   &lt;math&gt;
bentIdentity :: Floating a => a -> a

-- | Sigmoid-weighted linear unit. Multiply <a>logistic</a> by its input.
--   
--   &lt;math&gt;
siLU :: Floating a => a -> a

-- | SoftExponential
--   
--   To use with vectors (<a>R</a>), use <a>vmap'</a>.
--   
--   &lt;math&gt;
softExponential :: (Floating a, Ord a) => a -> a -> a

-- | Sinc
--   
--   &lt;math&gt;
sinc :: (Floating a, Eq a) => a -> a

-- | Gaussian
--   
--   &lt;math&gt;
gaussian :: Floating a => a -> a
tanh :: Floating a => a -> a
atan :: Floating a => a -> a
sin :: Floating a => a -> a

-- | Usable with functions like <a>*</a>, <a>isru</a>, etc. to turn them
--   into a form usable with <a>PFP</a>:
--   
--   <pre>
--   <a>liftUniform</a> (<a>*</a>)  :: <a>BVar</a> s <a>Double</a> -&gt; BVar s (<a>R</a> n) -&gt; BVar s (R n)
--   liftUniform <a>isru</a> :: BVar s Double -&gt; BVar s (R n) -&gt; BVar s (R n)
--   </pre>
--   
--   Basically turns a parmaeterized function on individual elements of
--   into one that shares the same parameter across all elements of the
--   vector.
liftUniform :: (Reifies s W, KnownNat n) => (BVar s (R n) -> r) -> BVar s Double -> r

-- | Inverse square root unit
--   
--   &lt;math&gt;
--   
--   See <a>liftUniform</a> to make this compatible with <a>PFP</a>.
--   
--   You can also just use this after partially applying it, to fix the
--   parameter (and not have it trained).
isru :: Floating a => a -> a -> a

-- | Parametric rectified linear unit
--   
--   To use with vectors (<a>R</a>), use <a>vmap'</a>.
--   
--   If scaling parameter is a fixed (and not learned) parameter, this is
--   typically called a leaky recitified linear unit (typically with α =
--   0.01).
--   
--   To use as a learned parameter:
--   
--   <pre>
--   <a>vmap</a> . <a>preLU</a> :: <a>BVar</a> s Double -&gt; <a>BVar</a> s (<a>R</a> n) -&gt; BVar s (R n)
--   </pre>
--   
--   This can be give directly to <a>PFP</a>.
--   
--   To fix the paramater ("leaky"), just partially apply a parameter:
--   
--   <pre>
--   <a>preLU</a> 0.01           :: <a>BVar</a> s (<a>R</a> n) -&gt; BVar s (R n)
--   preLU (<a>realToFrac</a> α) :: BVar s (R n) -&gt; BVar s (R n)
--   </pre>
--   
--   See also <tt>rreLU</tt>.
--   
--   &lt;math&gt;
preLU :: (Num a, Ord a) => a -> a -> a

-- | S-shaped rectified linear activiation unit
--   
--   See <a>sreLUPFP</a> for an uncurried and uniformly lifted version
--   usable with <a>PFP</a>.
--   
--   &lt;math&gt;
sreLU :: (Num a, Ord a) => a -> a -> a -> a -> a -> a

-- | An uncurried and uniformly lifted version of <a>sreLU</a> directly
--   usable with <a>PFP</a>.
sreLUPFP :: (KnownNat n, Reifies s W) => BVar s (T2 (T2 Double Double) (T2 Double Double)) -> BVar s (R n) -> BVar s (R n)

-- | Exponential linear unit
--   
--   To use as a learned parameter:
--   
--   <pre>
--   <a>vmap</a> . <a>eLU</a> :: <a>BVar</a> s Double -&gt; <a>BVar</a> s (<a>R</a> n) -&gt; BVar s (R n)
--   </pre>
--   
--   This can be give directly to <a>PFP</a>.
--   
--   To fix the paramater, just partially apply a parameter:
--   
--   <pre>
--   <a>eLU</a> 0.01 :: <a>BVar</a> s (<a>R</a> n) -&gt; BVar s (R n)
--   </pre>
--   
--   &lt;math&gt;
eLU :: (Floating a, Ord a) => a -> a -> a

-- | Inverse square root linear unit
--   
--   To use with vectors (<a>R</a>), use <a>vmap'</a>.
--   
--   &lt;math&gt;
isrLU :: (Floating a, Ord a) => a -> a -> a

-- | Adaptive piecewise linear activation unit
--   
--   See <a>aplPFP</a> for an uncurried version usable with <a>PFP</a>.
--   
--   &lt;math&gt;
apl :: (KnownNat n, KnownNat m, Reifies s W) => BVar s (L n m) -> BVar s (L n m) -> BVar s (R m) -> BVar s (R m)

-- | <a>apl</a> uncurried, to be directly usable with <a>PFP</a>.
aplPFP :: (KnownNat n, KnownNat m, Reifies s W) => BVar s (T2 (L n m) (L n m)) -> BVar s (R m) -> BVar s (R m)

-- | Softmax normalizer
softMax :: (KnownNat i, Reifies s W) => BVar s (R i) -> BVar s (R i)

-- | Maximum of vector.
--   
--   Compare to <a>norm_InfV</a>, which gives the maximum absolute value.
maxout :: (KnownNat n, Reifies s W) => BVar s (R n) -> BVar s Double
instance (GHC.Num.Num a, GHC.Num.Num b, Data.Type.Mayb.MaybeC GHC.Num.Num p, Data.Type.Mayb.KnownMayb p) => Backprop.Learn.Model.Class.Learn a b (Backprop.Learn.Model.Function.ParamFunc p a b)
instance (Data.Type.Mayb.MaybeC GHC.Num.Num p, Data.Type.Mayb.KnownMayb p) => Control.Category.Category (Backprop.Learn.Model.Function.ParamFunc p)

module Backprop.Learn.Model.Stochastic

-- | Dropout layer. Parameterized by dropout percentage (should be between
--   0 and 1).
--   
--   0 corresponds to no dropout, 1 corresponds to complete dropout of all
--   nodes every time.
newtype DO (n :: Nat)
DO :: Double -> DO
[_doRate] :: DO -> Double

-- | Represents a random-valued function, with a possible trainable
--   parameter.
--   
--   Requires both a "deterministic" and a "stochastic" mode. The
--   deterministic mode ideally should approximate some mean of the
--   stochastic mode.
data StochFunc :: Maybe Type -> Type -> Type -> Type
[SF] :: {_sfInitParam :: forall m. PrimMonad m => Gen (PrimState m) -> Mayb m p, _sfRunDeterm :: forall s. Reifies s W => Mayb (BVar s) p -> BVar s a -> BVar s b, _sfRunStoch :: forall m s. (PrimMonad m, Reifies s W) => Gen (PrimState m) -> Mayb (BVar s) p -> BVar s a -> m (BVar s b)} -> StochFunc p a b

-- | Convenient alias for a <a>StochFunc</a> (random-valued function with
--   both deterministic and stochastic modes) with no trained parameters.
type FixedStochFunc = StochFunc  'Nothing

-- | Construct a <a>FixedStochFunc</a>
_fsfRunDeterm :: () => FixedStochFunc a b -> forall s. Reifies s W => BVar s a -> BVar s b
_fsfRunStoch :: () => FixedStochFunc a b -> forall (m :: * -> *) s. (PrimMonad m, Reifies s W) => Gen PrimState m -> BVar s a -> m BVar s b

-- | Random leaky rectified linear unit
rreLU :: (ContGen d, Mean d, KnownNat n) => d -> FixedStochFunc (R n) (R n)

-- | Inject random noise. Usually used between neural network layers, or at
--   the very beginning to pre-process input.
--   
--   In non-stochastic mode, this adds the mean of the distribution.
injectNoise :: (ContGen d, Mean d, Fractional a) => d -> FixedStochFunc a a

-- | Multply by random noise. Can be used to implement dropout-like
--   behavior.
--   
--   In non-stochastic mode, this scales by the mean of the distribution.
applyNoise :: (ContGen d, Mean d, Fractional a) => d -> FixedStochFunc a a

-- | <a>injectNoise</a> lifted to <a>R</a>
injectNoiseR :: (ContGen d, Mean d, KnownNat n) => d -> FixedStochFunc (R n) (R n)

-- | <a>applyNoise</a> lifted to <a>R</a>
applyNoiseR :: (ContGen d, Mean d, KnownNat n) => d -> FixedStochFunc (R n) (R n)
instance Backprop.Learn.Model.Class.Learn a b (Backprop.Learn.Model.Stochastic.StochFunc p a b)
instance GHC.TypeNats.KnownNat n => Backprop.Learn.Model.Class.Learn (Internal.Static.R n) (Internal.Static.R n) (Backprop.Learn.Model.Stochastic.DO n)

module Data.Type.NonEmpty
data NETup :: NonEmpty Type -> Type
[NET] :: !a -> !(T as) -> NETup (a :| as)
netHead :: Functor f => (a -> f b) -> NETup (a :| as) -> f (NETup (b :| as))
netTail :: Functor f => (T as -> f (T bs)) -> NETup (a :| as) -> f (NETup (a :| bs))
unNet :: NETup (a :| as) -> (a, T as)
netT :: NETup (a :| as) -> T (a : as)
tNet :: T (a : as) -> NETup (a :| as)
instance (Control.DeepSeq.NFData a, Type.Family.List.ListC (Control.DeepSeq.NFData Type.Family.List.<$> as)) => Control.DeepSeq.NFData (Data.Type.NonEmpty.NETup (a 'GHC.Base.:| as))
instance (GHC.Num.Num a, Type.Family.List.ListC (GHC.Num.Num Type.Family.List.<$> as), Type.Class.Known.Known Data.Type.Length.Length as) => GHC.Num.Num (Data.Type.NonEmpty.NETup (a 'GHC.Base.:| as))

module Backprop.Learn.Model.Combinator

-- | Take a model and turn it into a model that runs its output into itself
--   several times, returning the final result. Parameterized by the number
--   of repeats, and the function to process the output to become the next
--   input.
--   
--   See <a>FeedbackTrace</a> if you want to observe all of the
--   intermediate outputs.
data Feedback :: Type -> Type -> Type -> Type
[FB] :: {_fbTimes :: Int, _fbFeed :: forall s. Reifies s W => BVar s b -> BVar s a, _fbLearn :: l} -> Feedback a b l

-- | Construct a <a>Feedback</a> from an endofunction (a function that
--   returns a value fo the same type as its input) by simply providing the
--   output directly as the next intput.
feedbackId :: Int -> l -> Feedback a a l

-- | Take a model and turn it into a model that runs its output into itself
--   several times, and returns all of the intermediate outputs.
--   Parameterized by the function to process the output to become the next
--   input.
--   
--   See <a>Feedback</a> if you only need the final result.
--   
--   Compare also to <tt>Unroll</tt>.
data FeedbackTrace :: Nat -> Type -> Type -> Type -> Type
[FBT] :: {_fbtFeed :: forall s. Reifies s W => BVar s b -> BVar s a, _fbtLearn :: l} -> FeedbackTrace n a b l

-- | Construct a <a>FeedbackTrace</a> from an endofunction (a function that
--   returns a value fo the same type as its input) by simply providing the
--   output directly as the next intput.
feedbackTraceId :: l -> FeedbackTrace n a a l

-- | Chain components linearly, retaining the ability to deconstruct at a
--   later time.
data Chain :: [Type] -> [Type] -> [Type] -> Type -> Type -> Type
[CNil] :: Chain '[] '[] '[] a a
[:~>] :: (Learn a b l, KnownMayb (LParamMaybe l), KnownMayb (LStateMaybe l)) => l -> Chain ls ps ss b c -> Chain (l : ls) (MaybeToList (LParamMaybe l) ++ ps) (MaybeToList (LStateMaybe l) ++ ss) a c

-- | Appending <a>Chain</a>
(~++) :: forall ls ms ps qs ss ts a b c. () => Chain ls ps ss a b -> Chain ms qs ts b c -> Chain (ls ++ ms) (ps ++ qs) (ss ++ ts) a c
chainParamLength :: Chain ls ps ss a b -> Length ps
chainStateLength :: Chain ls ps ss a b -> Length ss

-- | Data type representing trainable models.
--   
--   Useful for performant composition, but you lose the ability to
--   decompose parts.
data LearnFunc :: Maybe Type -> Maybe Type -> Type -> Type -> Type
[LF] :: {_lfInitParam :: forall m. PrimMonad m => Gen (PrimState m) -> Mayb m p, _lfInitState :: forall m. PrimMonad m => Gen (PrimState m) -> Mayb m s, _lfRunLearn :: forall q. Reifies q W => Mayb (BVar q) p -> BVar q a -> Mayb (BVar q) s -> (BVar q b, Mayb (BVar q) s), _lfRunLearnStoch :: forall m q. (PrimMonad m, Reifies q W) => Gen (PrimState m) -> Mayb (BVar q) p -> BVar q a -> Mayb (BVar q) s -> m (BVar q b, Mayb (BVar q) s)} -> LearnFunc p s a b
learnFunc :: Learn a b l => l -> LearnFunc (LParamMaybe l) (LStateMaybe l) a b

-- | Pre-compose a pure parameterless function to a model.
--   
--   An <tt><a>LMap</a> b a</tt> takes a model from <tt>b</tt> and turns it
--   into a model from <tt>a</tt>.
data LMap :: Type -> Type -> Type -> Type
[LM] :: {_lmPre :: forall s. Reifies s W => BVar s a -> BVar s b, _lmLearn :: l} -> LMap b a l

-- | Post-compose a pure parameterless function to a model.
--   
--   An <tt><tt>Rmap</tt> b c</tt> takes a model returning <tt>b</tt> and
--   turns it into a model returning <tt>c</tt>.
data RMap :: Type -> Type -> Type -> Type
[RM] :: {_rmPost :: forall s. Reifies s W => BVar s b -> BVar s c, _rmLearn :: l} -> RMap b c l

-- | Pre- and post-compose pure parameterless functions to a model.
--   
--   A <tt><a>Dimap</a> b c a d</tt> takes a model from <tt>b</tt> to
--   <tt>c</tt> and turns it into a model from <tt>a</tt> to <tt>d</tt>.
--   
--   <pre>
--   instance <a>Learn</a> b c =&gt; Learn a d (<a>Dimap</a> b c a d l) where
--       type <a>LParamMaybe</a> (Dimap b c a d l) = LParamMaybe l
--       type <a>LStateMaybe</a> (Dimap b c a d l) = LStateMaybe l
--   </pre>
type Dimap b c a d l = RMap c d (LMap b a l)

-- | Constructor for <a>Dimap</a>
_dmPre :: () => Dimap b c a d l -> forall s. Reifies s W => BVar s a -> BVar s b
_dmPost :: () => Dimap b c a d l -> forall s. Reifies s W => BVar s c -> BVar s d
_dmLearn :: () => Dimap b c a d l -> l

-- | Compose two <a>LearnFunc</a> on lists.
(.~) :: forall ps qs ss ts a b c. (ListC (Num <$> ps), ListC (Num <$> qs), ListC (Num <$> ss), ListC (Num <$> ts), ListC (Num <$> (ss ++ ts)), Known Length ps, Known Length qs, Known Length ss, Known Length ts) => LearnFunc ( 'Just (T ps)) ( 'Just (T ss)) b c -> LearnFunc ( 'Just (T qs)) ( 'Just (T ts)) a b -> LearnFunc ( 'Just (T (ps ++ qs))) ( 'Just (T (ss ++ ts))) a c

-- | Identity of <a>.~</a>
nilLF :: LearnFunc ( 'Just (T '[])) ( 'Just (T '[])) a a

-- | <a>LearnFunc</a> with singleton lists; meant to be used with <a>.~</a>
onlyLF :: forall p s a b. (KnownMayb p, MaybeC Num p, KnownMayb s, MaybeC Num s) => LearnFunc p s a b -> LearnFunc ( 'Just (T (MaybeToList p))) ( 'Just (T (MaybeToList s))) a b

-- | Compose two layers sequentially
--   
--   Note that this composes in the opposite order of <a>.</a> and
--   <tt>:.:</tt>, for consistency with the rest of the library.
data (:.~) :: Type -> Type -> Type
[:.~] :: l -> m -> l :.~ m
instance (Backprop.Learn.Model.Class.Learn a b l, GHC.TypeNats.KnownNat n, GHC.Num.Num b) => Backprop.Learn.Model.Class.Learn a (Data.Vector.Sized.Vector n b) (Backprop.Learn.Model.Combinator.FeedbackTrace n a b l)
instance Backprop.Learn.Model.Class.Learn a b l => Backprop.Learn.Model.Class.Learn a b (Backprop.Learn.Model.Combinator.Feedback a b l)
instance Backprop.Learn.Model.Class.Learn a b l => Backprop.Learn.Model.Class.Learn a c (Backprop.Learn.Model.Combinator.RMap b c l)
instance Backprop.Learn.Model.Class.Learn b c l => Backprop.Learn.Model.Class.Learn a c (Backprop.Learn.Model.Combinator.LMap b a l)
instance (Backprop.Learn.Model.Class.Learn a b l, Backprop.Learn.Model.Class.Learn b c m, Data.Type.Mayb.KnownMayb (Backprop.Learn.Model.Class.LParamMaybe l), Data.Type.Mayb.KnownMayb (Backprop.Learn.Model.Class.LParamMaybe m), Data.Type.Mayb.KnownMayb (Backprop.Learn.Model.Class.LStateMaybe l), Data.Type.Mayb.KnownMayb (Backprop.Learn.Model.Class.LStateMaybe m), Data.Type.Mayb.MaybeC GHC.Num.Num (Backprop.Learn.Model.Class.LParamMaybe l), Data.Type.Mayb.MaybeC GHC.Num.Num (Backprop.Learn.Model.Class.LParamMaybe m), Data.Type.Mayb.MaybeC GHC.Num.Num (Backprop.Learn.Model.Class.LStateMaybe l), Data.Type.Mayb.MaybeC GHC.Num.Num (Backprop.Learn.Model.Class.LStateMaybe m)) => Backprop.Learn.Model.Class.Learn a c (l Backprop.Learn.Model.Combinator.:.~ m)
instance Backprop.Learn.Model.Class.Learn a b (Backprop.Learn.Model.Combinator.LearnFunc p s a b)
instance (Data.Type.Mayb.MaybeC GHC.Num.Num p, Data.Type.Mayb.MaybeC GHC.Num.Num s, Data.Type.Mayb.KnownMayb p, Data.Type.Mayb.KnownMayb s) => Control.Category.Category (Backprop.Learn.Model.Combinator.LearnFunc p s)
instance (Type.Family.List.ListC (GHC.Num.Num Type.Family.List.<$> ps), Type.Family.List.ListC (GHC.Num.Num Type.Family.List.<$> ss)) => Backprop.Learn.Model.Class.Learn a b (Backprop.Learn.Model.Combinator.Chain ls ps ss a b)

module Backprop.Learn.Model.Neural

-- | Fully connected feed-forward layer with bias. Parameterized by its
--   initialization distribution.
--   
--   Note that this has no activation function; to use as a model with
--   activation function, chain it with an activation function using
--   <a>RMap</a>, <a>:.~</a>, etc.; see <a>FCA</a> for a convenient type
--   synonym and constructor.
--   
--   Without any activation function, this is essentially a multivariate
--   linear regression.
--   
--   With the logistic function as an activation function, this is
--   essentially multivariate logistic regression. (See <tt>Logistic</tt>)
newtype FC (i :: Nat) (o :: Nat)
FC :: forall m. PrimMonad m => Gen (PrimState m) -> m Double -> FC
[_fcGen] :: FC -> forall m. PrimMonad m => Gen (PrimState m) -> m Double

-- | Construct an <tt><a>FC</a> i o</tt> using a given distribution from
--   the <i>statistics</i> library.
fc :: ContGen d => d -> FC i o

-- | Fully connected feed-forward layer parameters.
data FCp i o
FCp :: !(R o) -> !(L o i) -> FCp i o
[_fcBias] :: FCp i o -> !(R o)
[_fcWeights] :: FCp i o -> !(L o i)
fcBias :: Lens' (FCp i o) (R o)
fcWeights :: Lens (FCp i o) (FCp i' o) (L o i) (L o i')

-- | Convenient synonym for an <a>FC</a> post-composed with a simple
--   parameterless activation function.
type FCA i o = RMap (R o) (R o) (FC i o)

-- | Construct an <a>FCA</a> using a generating function and activation
--   function.
--   
--   Some common ones include <tt>logistic</tt> and <tt><a>vmap</a>
--   <tt>reLU</tt></tt>.

-- | Construct an <tt><a>FCA</a> i o</tt> using a given distribution from
--   the <i>statistics</i> library.
fca :: ContGen d => d -> (forall s. Reifies s W => BVar s (R o) -> BVar s (R o)) -> FCA i o
_fcaGen :: () => FCA i o -> forall (m :: * -> *). PrimMonad m => Gen PrimState m -> m Double
_fcaActivation :: () => FCA i o -> forall s. Reifies s W => BVar s R o -> BVar s R o

-- | Fully connected recurrent layer with bias.
--   
--   Parameterized by its initialization distributions, and also the
--   function to compute the new state from previous input.
data FCR (h :: Nat) (i :: Nat) (o :: Nat)
FCR :: forall m. PrimMonad m => Gen (PrimState m) -> m Double -> forall m. PrimMonad m => Gen (PrimState m) -> m Double -> forall s. Reifies s W => BVar s (R o) -> BVar s (R h) -> FCR
[_fcrGen] :: FCR -> forall m. PrimMonad m => Gen (PrimState m) -> m Double
[_fcrGenState] :: FCR -> forall m. PrimMonad m => Gen (PrimState m) -> m Double
[_fcrStore] :: FCR -> forall s. Reifies s W => BVar s (R o) -> BVar s (R h)

-- | Construct an <tt><a>FCR</a> h i o</tt> using given distributions from
--   the <i>statistics</i> library.
fcr :: (ContGen d, ContGen e) => d -> e -> (forall s. Reifies s W => BVar s (R o) -> BVar s (R h)) -> FCR h i o

-- | Fully connected recurrent layer parameters.
data FCRp h i o
FCRp :: !(R o) -> !(L o i) -> !(L o h) -> FCRp h i o
[_fcrBias] :: FCRp h i o -> !(R o)
[_fcrInputWeights] :: FCRp h i o -> !(L o i)
[_fcrStateWeights] :: FCRp h i o -> !(L o h)
fcrBias :: Lens' (FCRp h i o) (R o)
fcrInputWeights :: Lens (FCRp h i o) (FCRp h i' o) (L o i) (L o i')
fcrStateWeights :: Lens (FCRp h i o) (FCRp h' i o) (L o h) (L o h')

-- | Convenient synonym for an <a>FCR</a> post-composed with a simple
--   parameterless activation function.
type FCRA h i o = RMap (R o) (R o) (FCR h i o)

-- | Construct an <a>FCRA</a> using a generating function and activation
--   function.
--   
--   Some common ones include <tt>logistic</tt> and <tt><a>vmap</a>
--   <tt>reLU</tt></tt>.

-- | Construct an <tt><a>FCRA</a> h i o</tt> using given distributions from
--   the <i>statistics</i> library.
fcra :: (ContGen d, ContGen e) => d -> e -> (forall s. Reifies s W => BVar s (R o) -> BVar s (R h)) -> (forall s. Reifies s W => BVar s (R o) -> BVar s (R o)) -> FCRA h i o
_fcraGen :: () => FCRA h i o -> forall (m :: * -> *). PrimMonad m => Gen PrimState m -> m Double
_fcraGenState :: () => FCRA h i o -> forall (m :: * -> *). PrimMonad m => Gen PrimState m -> m Double
_fcraStore :: () => FCRA h i o -> forall s. Reifies s W => BVar s R o -> BVar s R h
_fcraActivation :: () => FCRA h i o -> forall s. Reifies s W => BVar s R o -> BVar s R o
instance (GHC.TypeNats.KnownNat o, GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat h) => GHC.Show.Show (Backprop.Learn.Model.Neural.FCRp h i o)
instance GHC.Generics.Generic (Backprop.Learn.Model.Neural.FCRp h i o)
instance (GHC.TypeNats.KnownNat o, GHC.TypeNats.KnownNat i) => GHC.Show.Show (Backprop.Learn.Model.Neural.FCp i o)
instance GHC.Generics.Generic (Backprop.Learn.Model.Neural.FCp i o)
instance Control.DeepSeq.NFData (Backprop.Learn.Model.Neural.FCRp h i o)
instance (GHC.TypeNats.KnownNat h, GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Numeric.Opto.Update.Additive (Backprop.Learn.Model.Neural.FCRp h i o)
instance (GHC.TypeNats.KnownNat h, GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Numeric.Opto.Update.Scaling GHC.Types.Double (Backprop.Learn.Model.Neural.FCRp h i o)
instance (GHC.TypeNats.KnownNat h, GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Numeric.Opto.Update.Metric GHC.Types.Double (Backprop.Learn.Model.Neural.FCRp h i o)
instance (GHC.TypeNats.KnownNat h, GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o, Numeric.Opto.Ref.Ref m (Backprop.Learn.Model.Neural.FCRp h i o) v) => Numeric.Opto.Update.AdditiveInPlace m v (Backprop.Learn.Model.Neural.FCRp h i o)
instance (GHC.TypeNats.KnownNat h, GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o, Numeric.Opto.Ref.Ref m (Backprop.Learn.Model.Neural.FCRp h i o) v) => Numeric.Opto.Update.ScalingInPlace m v GHC.Types.Double (Backprop.Learn.Model.Neural.FCRp h i o)
instance (GHC.TypeNats.KnownNat h, GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => GHC.Num.Num (Backprop.Learn.Model.Neural.FCRp h i o)
instance (GHC.TypeNats.KnownNat h, GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => GHC.Real.Fractional (Backprop.Learn.Model.Neural.FCRp h i o)
instance (GHC.TypeNats.KnownNat h, GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => GHC.Float.Floating (Backprop.Learn.Model.Neural.FCRp h i o)
instance (GHC.TypeNats.KnownNat h, GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Backprop.Learn.Model.Class.Learn (Internal.Static.R i) (Internal.Static.R o) (Backprop.Learn.Model.Neural.FCR h i o)
instance Control.DeepSeq.NFData (Backprop.Learn.Model.Neural.FCp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Numeric.Opto.Update.Additive (Backprop.Learn.Model.Neural.FCp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Numeric.Opto.Update.Scaling GHC.Types.Double (Backprop.Learn.Model.Neural.FCp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Numeric.Opto.Update.Metric GHC.Types.Double (Backprop.Learn.Model.Neural.FCp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o, Numeric.Opto.Ref.Ref m (Backprop.Learn.Model.Neural.FCp i o) v) => Numeric.Opto.Update.AdditiveInPlace m v (Backprop.Learn.Model.Neural.FCp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o, Numeric.Opto.Ref.Ref m (Backprop.Learn.Model.Neural.FCp i o) v) => Numeric.Opto.Update.ScalingInPlace m v GHC.Types.Double (Backprop.Learn.Model.Neural.FCp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => GHC.Num.Num (Backprop.Learn.Model.Neural.FCp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => GHC.Real.Fractional (Backprop.Learn.Model.Neural.FCp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => GHC.Float.Floating (Backprop.Learn.Model.Neural.FCp i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Backprop.Learn.Model.Class.Learn (Internal.Static.R i) (Internal.Static.R o) (Backprop.Learn.Model.Neural.FC i o)

module Backprop.Learn.Model.Regression

-- | Multivariate linear regression, from an n-vector to an m-vector.
--   
--   Is essentially just a fully connected neural network layer with bias,
--   with no activation function.
type Linear n m = FC n m

-- | Construct a linear regression model from an initialization function
--   for coefficients
linearRegression :: (forall f. PrimMonad f => Gen (PrimState f) -> f Double) -> Linear n m
type Logistic n = RMap (R 1) Double (Linear n 1)

-- | Construct a logistic regression model from an initialization function
--   for coefficients.
logisticRegression :: (forall m. PrimMonad m => Gen (PrimState m) -> m Double) -> Logistic n

-- | Auto-regressive moving average model.
--   
--   ARMA(p,q) is an ARMA model with p autoregressive history terms and q
--   error (innovation) history terms.
--   
--   It is a <tt><a>Learn</a> Double Double</tt> instance, and is meant to
--   predict the "next step" of a time sequence.
--   
--   In this state, it is a runnable stateful model. To train, use with
--   <a>ARMAUnroll</a> to unroll and fix the initial state.
--   
--   An ARIMA (auto-regressive moving average model on differenced history)
--   model is a small modification from this; it is theoretically possible,
--   I just haven't gotten around to implementing it yet! Once done, we
--   could just define the type synonym <tt>type <a>ARMA</a> p q =
--   <tt>ARIMA</tt> p 0 q</tt>.
data ARMA :: Nat -> Nat -> Type
[ARMA] :: {_armaGenPhi :: forall m. PrimMonad m => Gen (PrimState m) -> Finite p -> m Double, _armaGenTheta :: forall m. PrimMonad m => Gen (PrimState m) -> Finite q -> m Double, _armaGenConst :: forall m. PrimMonad m => Gen (PrimState m) -> m Double, _armaGenYHist :: forall m. PrimMonad m => Gen (PrimState m) -> m Double, _armaGenEHist :: forall m. PrimMonad m => Gen (PrimState m) -> m Double} -> ARMA p q

-- | <a>ARMA</a> parmaeters
data ARMAp :: Nat -> Nat -> Type
[ARMAp] :: {_armaPhi :: !(R p), _armaTheta :: !(R q), _armaConstant :: !Double} -> ARMAp p q

-- | <a>ARMA</a> state
data ARMAs :: Nat -> Nat -> Type
[ARMAs] :: {_armaYPred :: !Double, _armaYHist :: !(R p), _armaEHist :: !(R q)} -> ARMAs p q

-- | The "unrolled" and "destated" <a>ARMA</a> model, which takes a vector
--   of sequential inputs and outputs a vector of the model's expected next
--   steps.
--   
--   Useful for actually training <a>ARMA</a> using gradient descent.
--   
--   This <i>fixes</i> the initial error history to be zero (or a fixed
--   stochastic sample), and treats the initial output history to be a
--   /learned parameter/.
--   
--   <pre>
--   instance <a>Learn</a> <a>Double</a> Double (<a>ARMAUnroll</a> p q) where
--       -- | Initial state is a parameter, but initial error history is fixed
--       type <a>LParamMaybe</a> (ARMAUnroll p q) = 'Just (T2 (ARMAp p q) (ARMAs p q))
--       type <a>LStateMaybe</a> (ARMAUnroll p q) = 'Nothing
--   </pre>
type ARMAUnroll p q = DeParamAt (T2 (ARMAp p q) (ARMAs p q)) (R q) (UnrollTrainState (Max p q) (ARMA p q))

-- | Constructor for <a>ARMAUnroll</a>
armaUnroll :: KnownNat q => ARMA p q -> ARMAUnroll p q

-- | Autoregressive model
type AR p = ARMA p 0

-- | Moving average model
type MA q = ARMA 0 q
instance (GHC.TypeNats.KnownNat a, GHC.TypeNats.KnownNat b) => GHC.Show.Show (Backprop.Learn.Model.Regression.ARMAs a b)
instance GHC.Generics.Generic (Backprop.Learn.Model.Regression.ARMAs a b)
instance (GHC.TypeNats.KnownNat a, GHC.TypeNats.KnownNat b) => GHC.Show.Show (Backprop.Learn.Model.Regression.ARMAp a b)
instance GHC.Generics.Generic (Backprop.Learn.Model.Regression.ARMAp a b)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat q, 1 GHC.TypeNats.<= p, 1 GHC.TypeNats.<= q) => Backprop.Learn.Model.Class.Learn GHC.Types.Double GHC.Types.Double (Backprop.Learn.Model.Regression.ARMA p q)
instance Control.DeepSeq.NFData (Backprop.Learn.Model.Regression.ARMAs p q)
instance GHC.Num.Num (Backprop.Learn.Model.Regression.ARMAs p q)
instance GHC.Real.Fractional (Backprop.Learn.Model.Regression.ARMAs p q)
instance GHC.Float.Floating (Backprop.Learn.Model.Regression.ARMAs p q)
instance Numeric.Opto.Update.Additive (Backprop.Learn.Model.Regression.ARMAs p q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat q) => Numeric.Opto.Update.Scaling GHC.Types.Double (Backprop.Learn.Model.Regression.ARMAs p q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat q) => Numeric.Opto.Update.Metric GHC.Types.Double (Backprop.Learn.Model.Regression.ARMAs p q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat q, Numeric.Opto.Ref.Ref m (Backprop.Learn.Model.Regression.ARMAs p q) v) => Numeric.Opto.Update.AdditiveInPlace m v (Backprop.Learn.Model.Regression.ARMAs p q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat q, Numeric.Opto.Ref.Ref m (Backprop.Learn.Model.Regression.ARMAs p q) v) => Numeric.Opto.Update.ScalingInPlace m v GHC.Types.Double (Backprop.Learn.Model.Regression.ARMAs p q)
instance Control.DeepSeq.NFData (Backprop.Learn.Model.Regression.ARMAp p q)
instance GHC.Num.Num (Backprop.Learn.Model.Regression.ARMAp p q)
instance GHC.Real.Fractional (Backprop.Learn.Model.Regression.ARMAp p q)
instance GHC.Float.Floating (Backprop.Learn.Model.Regression.ARMAp p q)
instance Numeric.Opto.Update.Additive (Backprop.Learn.Model.Regression.ARMAp p q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat q) => Numeric.Opto.Update.Scaling GHC.Types.Double (Backprop.Learn.Model.Regression.ARMAp p q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat q) => Numeric.Opto.Update.Metric GHC.Types.Double (Backprop.Learn.Model.Regression.ARMAp p q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat q, Numeric.Opto.Ref.Ref m (Backprop.Learn.Model.Regression.ARMAp p q) v) => Numeric.Opto.Update.AdditiveInPlace m v (Backprop.Learn.Model.Regression.ARMAp p q)
instance (GHC.TypeNats.KnownNat p, GHC.TypeNats.KnownNat q, Numeric.Opto.Ref.Ref m (Backprop.Learn.Model.Regression.ARMAp p q) v) => Numeric.Opto.Update.ScalingInPlace m v GHC.Types.Double (Backprop.Learn.Model.Regression.ARMAp p q)

module Backprop.Learn.Model
runLearn_ :: (Learn a b l, MaybeC Num (LStateMaybe l), Num b) => l -> LParam_ I l -> a -> LState_ I l -> (b, LState_ I l)
runLearnStoch_ :: (Learn a b l, MaybeC Num (LStateMaybe l), Num b, PrimMonad m) => l -> Gen (PrimState m) -> LParam_ I l -> a -> LState_ I l -> m (b, LState_ I l)
runLearnStateless_ :: (Learn a b l, NoState l) => l -> LParam_ I l -> a -> b
runLearnStochStateless_ :: (Learn a b l, NoState l, PrimMonad m) => l -> Gen (PrimState m) -> LParam_ I l -> a -> m b
gradLearn :: (Learn a b l, NoState l, Num a, Num b, MaybeC Num (LParamMaybe l)) => l -> LParam_ I l -> a -> (LParam_ I l, a)
gradLearnStoch :: (Learn a b l, NoState l, Num a, Num b, MaybeC Num (LParamMaybe l), PrimMonad m) => l -> Gen (PrimState m) -> LParam_ I l -> a -> m (LParam_ I l, a)

module Backprop.Learn.Train

-- | Gradient of model with respect to loss function and target
gradLearnLoss :: (Learn a b l, NoState l, LParamMaybe l ~  'Just (LParam l), Num (LParam l)) => Loss b -> l -> LParam l -> a -> b -> LParam l

-- | Stochastic gradient of model with respect to loss function and target
gradLearnStochLoss :: (Learn a b l, NoState l, LParamMaybe l ~  'Just (LParam l), Num (LParam l), PrimMonad m) => Loss b -> l -> Gen (PrimState m) -> LParam l -> a -> b -> m (LParam l)
type Grad (m :: * -> *) a = a -> m Diff a

-- | Using a model's deterministic prediction function (with a given loss
--   function), generate a <a>Grad</a> compatible with <a>Numeric.Opto</a>
--   and <a>Numeric.Opto.Run</a>.
learnGrad :: (MonadSample (a, b) m, Learn a b l, NoState l, LParamMaybe l ~  'Just (LParam l), Num (LParam l)) => Loss b -> l -> Grad m (LParam l)

-- | Using a model's stochastic prediction function (with a given loss
--   function), generate a <a>Grad</a> compatible with <a>Numeric.Opto</a>
--   and <a>Numeric.Opto.Run</a>.
learnGradStoch :: (MonadSample (a, b) m, PrimMonad m, Learn a b l, NoState l, LParamMaybe l ~  'Just (LParam l), Num (LParam l)) => Loss b -> l -> Gen (PrimState m) -> Grad m (LParam l)
consecutives :: Monad m => ConduitT i (i, i) m ()
consecutivesN :: forall v n i m. (KnownNat n, Vector v i, Monad m) => ConduitT i (Vector v n i, Vector v n i) m ()

module Backprop.Learn.Test
type Test o = o -> o -> Double
maxIxTest :: KnownNat n => Test (R n)
rmseTest :: KnownNat n => Test (R n)
squaredErrorTest :: Real a => Test a
totalSquaredErrorTest :: (Applicative t, Foldable t, Real a) => Test (t a)
squaredErrorTestV :: KnownNat n => Test (R n)
crossEntropyTest :: KnownNat n => Test (R n)
testLearn :: (Learn a b l, NoState l) => Test b -> l -> LParam_ I l -> a -> b -> Double
testLearnStoch :: (Learn a b l, NoState l, PrimMonad m) => Test b -> l -> Gen (PrimState m) -> LParam_ I l -> a -> b -> m Double
testLearnAll :: (Learn a b l, NoState l, Foldable t) => Test b -> l -> LParam_ I l -> t (a, b) -> Double
testLearnStochAll :: (Learn a b l, NoState l, PrimMonad m, Foldable t) => Test b -> l -> Gen (PrimState m) -> LParam_ I l -> t (a, b) -> m Double
instance (GHC.Base.Semigroup a, GHC.Base.Applicative m) => GHC.Base.Semigroup (Backprop.Learn.Test.M m a)
instance (GHC.Base.Monoid a, GHC.Base.Applicative m) => GHC.Base.Monoid (Backprop.Learn.Test.M m a)

module Backprop.Learn
