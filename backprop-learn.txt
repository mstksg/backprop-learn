-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Combinators and useful tools for ANNs using the backprop library
--   
--   See README.md
@package backprop-learn
@version 0.1.0.0

module Backprop.Learn

module Backprop.Learn.Function

module Data.Type.Mayb
data Mayb :: (k -> Type) -> Maybe k -> Type
[N_] :: Mayb f  'Nothing
[J_] :: f a -> Mayb f ( 'Just a)
fromJ_ :: Mayb f ( 'Just a) -> f a
data P :: k -> Type
[P] :: P a
zipMayb :: (forall a. f a -> g a -> h a) -> Mayb f m -> Mayb g m -> Mayb h m
zipMayb3 :: (forall a. f a -> g a -> h a -> i a) -> Mayb f m -> Mayb g m -> Mayb h m -> Mayb i m
instance forall k (f :: k -> *) (m :: GHC.Base.Maybe k). Data.Type.Mayb.MaybeC * GHC.Show.Show ((Data.Type.Mayb.<$>) k * f m) => GHC.Show.Show (Data.Type.Mayb.Mayb k f m)
instance forall k1 (k2 :: k1). Type.Class.Known.Known k1 (Data.Type.Mayb.P k1) k2
instance forall k (m :: GHC.Base.Maybe k) (f :: k -> *). (Type.Class.Known.Known (GHC.Base.Maybe k) (Data.Type.Mayb.Mayb k (Data.Type.Mayb.P k)) m, Data.Type.Mayb.MaybeC * GHC.Num.Num ((Data.Type.Mayb.<$>) k * f m)) => GHC.Num.Num (Data.Type.Mayb.Mayb k f m)
instance forall k (m :: GHC.Base.Maybe k) (f :: k -> *). (Type.Class.Known.Known (GHC.Base.Maybe k) (Data.Type.Mayb.Mayb k (Data.Type.Mayb.P k)) m, Data.Type.Mayb.MaybeC * GHC.Num.Num ((Data.Type.Mayb.<$>) k * f m), Data.Type.Mayb.MaybeC * GHC.Real.Fractional ((Data.Type.Mayb.<$>) k * f m)) => GHC.Real.Fractional (Data.Type.Mayb.Mayb k f m)
instance forall k (f :: k -> *). Type.Class.Known.Known (GHC.Base.Maybe k) (Data.Type.Mayb.Mayb k f) ('GHC.Base.Nothing k)
instance forall a1 (f :: a1 -> *) (a2 :: a1). Type.Class.Known.Known a1 f a2 => Type.Class.Known.Known (GHC.Base.Maybe a1) (Data.Type.Mayb.Mayb a1 f) ('GHC.Base.Just a1 a2)
instance Type.Class.Higher.Functor1 k (GHC.Base.Maybe k) (Data.Type.Mayb.Mayb k)

module Backprop.Learn.Class

-- | Class for models that can be trained using gradient descent
--   
--   An instance <tt>l</tt> of <tt><a>Learn</a> a b</tt> is parameterized
--   by <tt>p</tt>, takes <tt>a</tt> as input, and returns <tt>b</tt> as
--   outputs. <tt>l</tt> can be thought of as a value containing the
--   <i>hyperparmaeters</i> of the model.
class (MaybeC Num (LParamMaybe l), MaybeC Num (LStateMaybe l), Num a, Num b, Known (Mayb P) (LParamMaybe l), Known (Mayb P) (LStateMaybe l)) => Learn a b l | l -> a, l -> b where {
    type family LParamMaybe l :: Maybe Type;
    type family LStateMaybe l :: Maybe Type;
    type LParamMaybe l =  'Nothing;
    type LStateMaybe l =  'Nothing;
}

-- | Initialize parameters, given the hyperparameters in <tt>l</tt>.
initParam :: (Learn a b l, PrimMonad m) => l -> Gen (PrimState m) -> LParam m l

-- | Initialize parameters, given the hyperparameters in <tt>l</tt>.
initParam :: (Learn a b l, (LParamMaybe l ~  'Nothing)) => l -> Gen (PrimState m) -> LParam m l

-- | Initialize state, given the hyperparameters in <tt>l</tt>.
initState :: (Learn a b l, PrimMonad m) => l -> Gen (PrimState m) -> LState m l

-- | Initialize state, given the hyperparameters in <tt>l</tt>.
initState :: (Learn a b l, (LStateMaybe l ~  'Nothing)) => l -> Gen (PrimState m) -> LState m l

-- | Run the model itself, deterministically.
--   
--   If your model has no state, you can define this conveniently using
--   <a>stateless</a>.
runLearn :: (Learn a b l, Reifies s W) => l -> LParam (BVar s) l -> BVar s a -> LState (BVar s) l -> (BVar s b, LState (BVar s) l)

-- | Run a model in stochastic mode.
--   
--   If model is inherently non-stochastic, a default implementation is
--   given in terms of <a>runLearn</a>.
--   
--   If your model has no state, you can define this conveniently using
--   <tt>statelessStoch</tt>.
runLearnStoch :: (Learn a b l, Reifies s W, PrimMonad m) => l -> Gen (PrimState m) -> LParam (BVar s) l -> BVar s a -> LState (BVar s) l -> m (BVar s b, LState (BVar s) l)

-- | The trainable parameter type of a model. Will be a compile-time error
--   if the model has no trainable parameters.
type LParamOf l = FromJust ( 'ShowType l :<>:  'Text " has no trainable parameters") (LParamMaybe l)

-- | The state type of a model. Will be a compile-time error if the model
--   has no state.
type LStateOf l = FromJust ( 'ShowType l :<>:  'Text " has no trainable parameters") (LStateMaybe l)

-- | Constraint specifying that a given model has no trainabale parameters.
type NoParam l = LParamMaybe l ~  'Nothing

-- | Constraint specifying that a given model has no state.
type NoState l = LStateMaybe l ~  'Nothing

-- | Is <a>N_</a> if there is <tt>l</tt> has no trainable parameters;
--   otherwise is <a>J_</a> with <tt>f p</tt>, for trainable parameter type
--   <tt>p</tt>.
type LParam f l = Mayb f (LParamMaybe l)

-- | Is <a>N_</a> if there is <tt>l</tt> has no state; otherwise is
--   <a>J_</a> with <tt>f s</tt>, for state type <tt>s</tt>.
type LState f l = Mayb f (LStateMaybe l)

-- | Useful for defining <a>runLearn</a> if your model has no state.
stateless :: (a -> b) -> (a -> s -> (b, s))

-- | Useful for defining <a>runLearnStoch</a> if your model has no state.
statelessM :: Functor m => (a -> m b) -> (a -> s -> m (b, s))
runLearnStateless :: (Learn a b l, Reifies s W, NoState l) => l -> LParam (BVar s) l -> BVar s a -> BVar s b
runLearnStochStateless :: (Learn a b l, Reifies s W, NoState l, PrimMonad m) => l -> Gen (PrimState m) -> LParam (BVar s) l -> BVar s a -> m (BVar s b)
data Mayb :: (k -> Type) -> Maybe k -> Type
[N_] :: Mayb f  'Nothing
[J_] :: f a -> Mayb f ( 'Just a)
fromJ_ :: Mayb f ( 'Just a) -> f a

module Backprop.Learn.FullyConnected

-- | Fully connected feed-forward layer with bias. Parameterized by its
--   initialization distribution.
newtype FC (i :: Nat) (o :: Nat)
FC :: (forall m. PrimMonad m => Gen (PrimState m) -> m Double) -> FC
[_fcGen] :: FC -> forall m. PrimMonad m => Gen (PrimState m) -> m Double

-- | Construct an <tt><a>FC</a> i o</tt> using a given distribution from
--   the <i>statistics</i> library.
fc :: ContGen d => d -> FC i o

-- | Fully connected feed-forward layer parameters.
data FCP i o
FCP :: !(R o) -> !(L o i) -> FCP i o
[_fcBias] :: FCP i o -> !(R o)
[_fcWeights] :: FCP i o -> !(L o i)
fcBias :: Functor f => (R o -> f (R o)) -> FCP i o -> f (FCP i o)
fcWeights :: Functor f => (L o i -> f (L o k)) -> FCP i o -> f (FCP k o)
instance GHC.Generics.Generic (Backprop.Learn.FullyConnected.FCP i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => GHC.Num.Num (Backprop.Learn.FullyConnected.FCP i o)
instance (GHC.TypeNats.KnownNat i, GHC.TypeNats.KnownNat o) => Backprop.Learn.Class.Learn (Internal.Static.R i) (Internal.Static.R o) (Backprop.Learn.FullyConnected.FC i o)

module Backprop.Learn.Dropout

-- | Dropout layer. Parameterized by dropout percentage (should be between
--   0 and 1).
--   
--   0 corresponds to no dropout, 1 corresponds to complete dropout of all
--   nodes every time.
newtype DO (n :: Nat)
DO :: Double -> DO
[_doRate] :: DO -> Double
instance GHC.TypeNats.KnownNat n => Backprop.Learn.Class.Learn (Internal.Static.R n) (Internal.Static.R n) (Backprop.Learn.Dropout.DO n)
